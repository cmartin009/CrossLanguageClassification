Label,ID,Text
French,article-0-0-fr,"Résumé 
Dans ce papier, nous proposons une méthode pour résumer et interroger des logs de requêtes OLAP. L’idée de base est qu’une requête résume une autre requête et qu’un log, qui est une séquence de requêtes, résume un autre log. Notre cadre formel est composé d’une algèbre simple destinée à résumer des requêtes OLAP, et d’une mesure évaluant la qualité du résumé obtenu. Nous proposons également plusieurs stratégies pour calculer automatiquement des résumés de logs de bonne qualité, et nous montrons comment des propriétés simples sur les résumés peuvent être utilisées pour interroger un log efficacement. Des tests sur des logs de requêtes MDX ont montré l’intérêt de notre approche."
English,article-0-0-eng,"Abstract In this paper, we propose a method for summarizing OLAP query logs. The basic idea is that a query summarizes another query and that a log, which is a sequence of queries, summarizes another log. Our framework allows to declaratively specify a summary, and includes a measure to assess the quality of the summaries.We also propose several strategies for automatically computing a good quality summary of a query log, and we show how some simple properties on the summaries can be used to query the log efficiently. Tests on MDX query logs showed the usefulness of our approach."
TEng,article-0-0-teng,"Summary
In this paper, we propose a method for summarizing and interrogating OLAP request logs. The basic idea is that a request summarizes another request and that a log, which is a sequence of requests, summarizes another log. Our formal framework is composed of a simple algebra intended to summarize OLAP queries, and of a measure evaluating the quality of the summary obtained. We also offer several strategies for automatically calculating good quality log summaries, and we show how simple properties on summaries can be used to query a log efficiently. Tests on MDX request logs have shown the interest of our approach."
French,article-0-14-fr,"Intuition
Nous cherchons à évaluer si une requête (resp., un log), qui correspond à un ensemble de références (resp., requêtes), résume fidèlement une autre requête (resp., un autre log). Les opérateurs de QSL résument en conservant, ajoutant ou retirant des références aux requêtes à résumer pour constituer le résumé. Une manière d’évaluer cette fidélité est de mesurer la proportion de ce qui est ajouté ou enlevé par l’opérateur. Nous nous sommes donc tournés vers les mesures de rappel et précision classiques en recherche d’information."
English,article-0-14-eng,"Intuition
The measure should assess to which extends a query (respectively, a log), which is a set of references (respectively, of queries), is a faithful summary of another query (respectively, another log). The operators of QSL define summaries by adding or removing references to their operands. For instance the lca operator summarizes by adding references containing ancestors. The measure should thus assess the proportion of what is added or removed to define the summary. This is achieved by adapting the classical notion of precision and recall."
TEng,article-0-14-teng,"Intuition
We seek to assess whether a request (resp., A log), which corresponds to a set of references (resp., Requests), faithfully summarizes another request (resp., Another log). The operators of QSL summarize by preserving, adding or removing references to the queries to be summarized to constitute the summary. One way to assess this fidelity is to measure the proportion of what is added or removed by the operator. We therefore turned to conventional recall and precision measurements in search of information."
French,article-0-16-fr,"Ainsi, nous proposons d’étendre rappel et précision par la prise en compte d’une relation de couverture. Pour être le plus général possible, nous choisissons la relation de couverture définie sur les références puisque tant les requêtes que les logs peuvent être assimilés à des ensembles de références. Ainsi définie de manière générale, cette mesure présente l’intérêt de pouvoir être appliquée sur deux requêtes ou deux logs, ou n’importe quel couple d’ensembles de références."
English,article-0-16-eng,"We propose to extend recall and precision by taking into account a cover relation between the elements of the two sets, the summary and the summarized. In this article we use the cover relation defined over references since both queries and logs can be seen as sets of references, and thus the quality measure can be used on queries or on logs, or on any sets of references. Note that the definition of the measure is even more general in the sense that it does not rely on a particular cover relation. We now formalize these notions."
TEng,article-0-16-teng,"Thus, we propose to extend recall and precision by taking into account a hedging relationship. To be as general as possible, we choose the coverage relationship defined on the references since both the queries and the logs can be assimilated to sets of references. Thus defined in general, this measure has the advantage of being able to be applied to two requests or two logs, or any couple of sets of references."
French,article-0-2-fr,"Dans cet article, nous développons les travaux entrepris sur le résumé de log de requêtes OLAP (Aligon et al., 2010a), où nous proposions un langage de manipulation de requêtes (appelé QSL) pour résumer des requêtes OLAP, une mesure de la qualité d’un résumé et un algorithme glouton de construction automatique de résumés de log de requêtes utilisant QSL. Sur cette base, nous proposons un nouvel algorithme de calcul de résumés et deux sous-langages de QSL dont nous étudions les propriétés. Nous indiquons comment ces propriétés sont utilisées pour interroger un log. Finalement, nous présentons l’implémentation de notre cadre et quelques tests illustrant son efficacité."
English,article-0-2-eng,"In this article we present and develop the work initiated in [1, 2]. In these papers, we proposed a framework for summarizing an OLAP query log, and we studied basic properties of the framework for helping the user to query the log. The present article provides a detailed presentation of the framework and introduces its implementation as a system for summarizing and querying log files. To this end, we extend the search facilities introduced in [2] to obtain a declarative language with which complex queries over a log file can be expressed."
TEng,article-0-2-teng,"In this article, we develop the work undertaken on the OLAP request log summary (Aligon et al., 2010a), where we proposed a request manipulation language (called QSL) to summarize OLAP requests, a measure of quality. of a summary and a greedy algorithm of automatic construction of summaries of request log using QSL. On this basis, we propose a new algorithm for calculating summaries and two QSL sublanguages whose properties we are studying. We indicate how these properties are used to query a log. Finally, we present the implementation of our framework and some tests illustrating its effectiveness."
French,article-0-4-fr,"Dans la section 5, nous présentons notre nouvel algorithme de résumé et introduisons quelques propriétés des résumés obtenus à l’aide des sous-langages de QSL. La section 6 présente l’exploitation des propriétés de notre cadre pour interroger des logs. La section 7 présente notre implémentation et les tests conduits. Nous discutons des travaux connexes et concluons dans la section 8."
English,article-0-4-eng,"In Section 5, we present the algorithm that automatically constructs summaries based on QSL and the quality measure. We also introduce the properties of the summaries constructed with sub-languages of QSL. Section 6 presents the language for querying logs, and describes how properties of the framework can be used to ensure efficient searches. Section 7 describes the implementation of the framework and the experiments conducted to evaluate its effectiveness. Section 8 discusses related work. We conclude and draw perspectives in Section 9."
TEng,article-0-4-teng,"In section 5, we present our new summary algorithm and introduce some properties of summaries obtained using QSL sublanguages. Section 6 presents the exploitation of the properties of our framework to query logs. Section 7 presents our implementation and the tests conducted. We discuss related work and conclude in section 8."
French,article-0-7-fr,"Tout d’abord notons que plusieurs manières de résumer sont pertinentes. Par exemple, en résumant le log par la seule requête mentionnant les membres les plus fréquemment interrogés, ce qui intéressera un administrateur souhaitant savoir quels indexes positionner. Dans notre exemple une telle requête demanderait les ventes de Coke dans les régions North et South pour July 2008 (la requête q2). Une autre manière de résumer permettrait à un utilisateur de savoir grossièrement ce qui a été fait sur le cube, sous la forme d’une requête interrogeant les ventes de soda en France en 2008. Si l’utilisateur est intéressé par plus de détails, il pourra ensuite interroger le log pour trouver des requêtes précises. Un autre intérêt d’un résumé, plus compact qu’un log, est donc de pouvoir être utilisé en amont du log pour rendre l’interrogation plus efficace."
English,article-0-7-eng,"Assume we want to summarize these queries by another query. Various solutions are possible. First, we can summarize the queries by retaining for each dimension the most frequent members. This could be of interest for a DBA who would like to know what indices to store. In that case, the resulting query would ask for sales of Coke in regions North or South during July 2008 (i.e., query q2). A second alternative would be to summarize the queries with another query having for each dimension the members that cover all members present in the initial queries. For example, note that Pepsi, Coke and Orangina are sodas, cities Paris and Marseille and regions North and South are in France and all three queries concern year 2008. The query summarizing the log L would then ask for the sales of Soda in France in 2008. The user interested in more details on the query could then query the log to find the queries that were indeed launched."
TEng,article-0-7-teng,"First, note that several ways of summarizing are relevant. For example, by summarizing the log by the only query mentioning the members most frequently questioned, which will interest an administrator wishing to know which indexes to position. In our example, such a query would request sales of Coke in the North and South regions for July 2008 (query q2). Another way of summarizing would allow a user to know roughly what was done on the cube, in the form of a query querying sales of soda in France in 2008. If the user is interested in more details, he can then query the log to find specific requests. Another advantage of a summary, more compact than a log, is therefore that it can be used upstream of the log to make the query more efficient."
French,article-1-12-fr,"Une brève description des données est reportée en table 1. L’expérience “train-test” consiste à diviser la base de données en deux sous-ensembles en respectant la distribution des classes. La première sert à l’apprentissage (i.e. l’extraction de règles selon des seuils donnés de fréquence, confiance et taux d’accroissement), la deuxième sert à évaluer l’évolution des valeurs des mesures (en test). Nous calculons et comparons aussi les valeurs de level des règles extraites en apprentissage et en test."
English,article-1-12-eng,"The train-test experiments consist in dividing a data set in two (almost) equal class-stratified parts. One part is for training and mining frequent-confident (or emerging) rules, the other part is for evaluating the evolution of confidence and growth rate values on the test set. Since we do not provide an extractor of MODL rules in this preliminary work, we compute the value of our MODL criterion for the extracted confident (or emerging) rules on the training and test set for comparison."
TEng,article-1-12-teng,"A brief description of the data is given in table 1. The “train-test” experiment consists in dividing the database into two subsets while respecting the distribution of the classes. The first is used for learning (i.e. the extraction of rules according to given thresholds of frequency, confidence and rate of increase), the second is used to evaluate the evolution of the values of the measures (in test). We also compute and compare the level values of the rules extracted in learning and testing."
French,article-1-14-fr,"Données originales. Dans les graphiques de la figure 2, nous reportons l’évolution train-test des valeurs de taux d’acroissement (GR) pour chaque base de données. Nous comparons aussi les valeurs de level des règles extraites. Nous remarquons que GR est généralement instable : en effet, une règle à fort taux d’accroissement en apprentissage peut avoir un faible GR en test (voir les points éloignés de la droite identité) – ce qui confirme notre hypothèse que le taux d’accroissement (comme la confiance) ne capturent pas la notion de robustesse. Au contraire les valeurs de level sont très stables (points proche de l’identité)."
English,article-1-14-eng,"Original data sets. In figures 2 and 3, we report scatter plots for the study of the evolution (from train set to test set) of confidence values of extracted rules. We also compare the values of the MODL criterion. As expected, for all data sets, we observe that confidence is unstable from train to test: indeed, a highly confident rule in train may have low confidence in test (see the points far from the identity line). Conversely, the MODL level values of extracted rules are rather stable in the traintest experiments (see the points close to the identity line)."
TEng,article-1-14-teng,"Original data. In the graphs in Figure 2, we report the train-test evolution of the growth rate (GR) values for each database. We also compare the level values of the extracted rules. We note that GR is generally unstable: indeed, a rule with a high rate of increase in learning can have a low GR in test (see the points far from the right identity) - which confirms our hypothesis that the rate of increase (like trust) do not capture the notion of robustness. On the contrary, the level values are very stable (points close to identity)."
French,article-1-16-fr,"Données bruitées. Afin de simuler la présence de bruit de classe dans les données breast-w, nous ajoutons de manière uniforme du bruit à l’attribut classe (changement de classe) en utilisant la fonction AddNoise de WEKA (Witten et Frank, 2005). Nous utilisons deux niveaux de bruit : moyen (20%) et fort (50%). Nous renouvelons l’expérience train-test sur chaque version artificiellement bruitée. Les résultats sont reportés en figure 5. A chaque niveau de bruit, les extracteurs classiques réussissent à extraire des motifs “potentiellement” intéressants – notons tout de même que beaucoup moins de règles sont extraites des contextes fortement bruités. Cependant, l’expérience train-test montre encore une fois l’instabilité des mesures classiques et cette instabilité est d’autant plus grande lorsque le contexte est très bruité (50%)."
English,article-1-16-eng,"Noisy data. In order to simulate the presence of class noise in breast-w data, we uniformly add noise to the class attribute (class change) using WEKA's AddNoise function (Witten and Frank, 2005). We use two noise levels: medium (20%) and loud (50%). We renew the train-test experience on each artificially noised version. The results are shown in Figure 5. At each noise level, the classic extractors manage to extract “potentially” interesting patterns - note however that far fewer rules are extracted from highly noisy contexts. However, the train-test experience once again shows the instability of conventional measurements and this instability is even greater when the context is very noisy (50%)."
TEng,article-1-16-teng,"Noisy data. In order to simulate the presence of class noise in breast-w data, we uniformly add noise to the class attribute (class change) using WEKA's AddNoise function (Witten and Frank, 2005). We use two noise levels: medium (20%) and loud (50%). We renew the train-test experience on each artificially noised version. The results are shown in Figure 5. At each noise level, the classic extractors manage to extract “potentially” interesting patterns - note however that far fewer rules are extracted from highly noisy contexts. However, the train-test experience once again shows the instability of conventional measurements and this instability is even greater when the context is very noisy (50%)."
French,article-1-2-fr,"Le paramétrage. Le seuillage de la mesure d’intérêt utilisée est une étape cruciale et pour autant non-triviale. Le dilemme est bien connu : un seuil de fréquence minimum élevé génère moins de règles mais aussi un faible taux de couverture des données et souvent moins de pouvoir discriminant pour les classes du problème. D’un autre côté, un seuil de fréquence très bas génère un grand nombre de règles parmi lesquelles certaines (de faible fréquence) peuvent être erronées."
English,article-1-2-eng,"The Curse of Parameters. The choice of parameter values is crucial but not trivial. The dilemma is well-known: a high frequency threshold may lead to less rules, but also lesser coverage rate and less discriminating power. A low frequency threshold may lead to a huge amount of rules, among which some rules (with low frequency) may be spurious."
TEng,article-1-2-teng,"Setting. The thresholding of the measure of interest used is a crucial and non-trivial step. The dilemma is well known: a high minimum frequency threshold generates fewer rules but also a low data coverage rate and often less discriminating power for the classes of the problem. On the other hand, a very low frequency threshold generates a large number of rules, some of which (of low frequency) may be wrong."
French,article-1-21-fr,"A propos du principe MDL. Siebes et al. (2006) propose une approche d’extraction de motifs basée sur le principe MDL. Les auteurs cherchent à extraire les itemsets qui fournissent une bonne compression des données. Le lien entre probabilités et longueurs de codage permet aux auteurs de réécrire la longueur de codage d’un itemset I en -log(P(I)). Ainsi, les “meilleurs” itemsets ont un codage plus court et compressent mieux les données. Dans (van Leeuwen et al., 2006), une extension pour la classification supervisée est proposée. Les deux principales différences avec l’approche MODL sont les suivantes : (i), l’utilisation de l’apriori hiérarchique MODL implique un codage différent ; (ii), Siebes et al. (2006) cherchent un ensemble de motifs qui compressent les données alors qu’ici notre critère est défini pour une règle."
English,article-1-21-eng,"About MDL. In [Siebes et al., 2006], the authors develop a MDL-based pattern mining approach. The authors look for itemsets that provides a good compression of the data. The link between probability and codes allow them to rewrite the code length of an item set I as -log(P(I)). Thus, the best item sets have shortest codes. In [van Leeuwen et al., 2006], an extension for classification purpose is suggested. The two main differences with the MODL approach are : (i) the use of the MODL hierarchical prior implies a different way of coding information ; (ii) in [van Leeuwen et al., 2006], authors look for a set of patterns to compress the data whereas our MODL criterion is defined for one rule."
TEng,article-1-21-teng,"About the MDL principle. Siebes et al. (2006) proposes an approach to extracting patterns based on the MDL principle. The authors seek to extract the itemsets which provide good data compression. The link between probabilities and coding lengths allows authors to rewrite the coding length of an itemset I in -log (P (I)). Thus, the “best” itemsets have a shorter coding and compress the data better. In (van Leeuwen et al., 2006), an extension for supervised classification is proposed. The two main differences with the MODL approach are as follows: (i), the use of the MODL hierarchical overview implies different coding; (ii), Siebes et al. (2006) look for a set of patterns that compress the data while here our criterion is defined for a rule."
French,article-1-3-fr,"Le même dilemme persiste pour le seuillage de mesures d’intérêt telles que la confiance (i.e. une estimation de la probabilité P(c | X)) ou le taux d’accroissement (qui permet d’identifier les motifs émergents, i.e., qui sont fréquents dans une classe de données et infréquents dans le reste de la base (Dong et Li, 1999)). En effet, des seuils élevés de confiance (ou de taux d’accroissement) génèrent un petit nombre de règles de classification presque pures qui sont rares (voire erronées si combinées avec un seuil de fréquence bas) alors que des seuils bas génèrent beaucoup de règles avec un intérêt limité. Ainsi, trouver un compromis entre seuil de fréquence et mesure d’intérêt n’est pas trivial."
English,article-1-3-eng,"The same dilemma stands when thresholding interestingness measures like confidence (i.e. an estimation of the probability P(c | X)) or growth rate (which highlights the so-called emerging patterns, i.e. those patterns that frequent in a class of the data set and barely infrequent in the rest of the data [Dong and Li, 1999]): indeed, high confidence (or growth rate) threshold values lead to strong (pure) class association rules which may be rare in real-world data or even wrong when combined with a low frequency threshold whereas “low” thresholds generate a lot of rules with limited interest. Thus, finding a trade-off between frequency and interestingness measure values is not trivial."
TEng,article-1-3-teng,"The same dilemma persists for the thresholding of measures of interest such as confidence (ie an estimate of the probability P (c | X)) or the rate of increase (which makes it possible to identify the emerging patterns, ie, which are frequent in a data class and infrequent in the rest of the database (Dong and Li, 1999)). Indeed, high confidence (or rate of increase) thresholds generate a small number of almost pure classification rules which are rare (even erroneous if combined with a low frequency threshold) while low thresholds generate a lot of rules with limited interest. Thus, finding a compromise between frequency threshold and interest measure is not trivial."
French,article-1-4-fr,"L’instabilité des mesures d’intérêt. Bien que des sous-ensembles de règles permettent de bonnes prédictions, il est facile de montrer que des règles à forte confiance ou émergentes ne sont pas individuellement robustes. Dans la figure 1, nous comparons les valeurs de confiance (resp. de taux d’accroissement et de lift (Brin et al., 1997)) en apprentissage et en test de règles extraites de la base UCI breast-w (Asuncion et Newman, 2007)."
English,article-1-4-eng,"Instability of interestingness measures. Even if subsets of extracted rules have
 shown to be quite effective for predictions, it can be easily shown that highly confident
 or emerging rules are not individually robust. In figure 1, we compare the confidence
 (resp. growth rate) train values with the confidence (resp. growth rate) test values
 of rules extracted from UCI breast-w data set [Asuncion and Newman, 2007]."
TEng,article-1-4-teng,"Instability of interest measures. Although subsets of rules allow good predictions, it is easy to show that high-confidence or emerging rules are not individually robust. In Figure 1, we compare the confidence values (resp. Growth and lift rates (Brin et al., 1997)) in learning and testing rules extracted from the UCI breast-w database (Asuncion and Newman , 2007)."
French,article-10-1-fr,"Ces transducteurs, capturant les grandes régularités de traduction existant dans le domaine biomédical, sont ensuite utilisés pour traduire de nouveaux termes français en anglais et vice versa. Les évaluations menées montrent que le taux de bonnes traductions de notre technique se situe entre 52 et 67%. À travers un examen des erreurs les plus courantes, nous identifions quelques limites inhérentes à notre approche et proposons quelques pistes pour les dépasser. Nous envisageons enfin plusieurs extensions à ce travail."
English,article-10-1-eng,"Then, these transducers, making the most of high translation regularities in the biomedical domain, can be used to translate new French terms into English or vice versa. Evaluations reported show that our technique achieves good successful translation rates (between 52 and 67%). When examining at the most frequent errors made, some inherent limits of our approach are identified, and several avenues are proposed in order to bypass them. Finally, some perspectives are put forward to extend this work."
TEng,article-10-1-teng,"These transducers, capturing the main translation regularities existing in the biomedical field, are then used to translate new French terms into English and vice versa. Evaluations have shown that the rate of good translations of our technique is between 52 and 67%. Through an examination of the most common errors, we identify some limits inherent in our approach and suggest some ways to overcome them. We are finally considering several extensions to this work."
French,article-10-10-fr,"2. tous les préfixes communs des séquences de sortie sont ensuite remontés des feuilles vers la racine de l’arbre (figure 3).
3. enfin, en partant de la racine, tous les noeuds sont considérés deux à deux et fusionnés si le transducteur résultant n’entre pas en contradiction avec les données du jeu d’entraînement (figures 4 et 5). L’ordre de ces tentatives de fusion est généralement indiqué par une fonction heuristique. Il est possible de repousser des séquences de sortie vers les feuilles pour permettre des fusions. Quand plus aucune fusion n’est possible, l’algorithme termine."
English,article-10-10-eng,"2. all the common prefixes of the output sequences are then raised from the leaves to the root of the tree (Figure 3).
3. finally, starting from the root, all the nodes are considered in pairs and merged if the resulting transducer does not contradict the data of the training set (Figures 4 and 5). The order of these merging attempts is usually indicated by a heuristic function. It is possible to push output sequences towards the sheets to allow mergers. When no more merging is possible, the algorithm ends."
TEng,article-10-10-teng,"2. all the common prefixes of the output sequences are then raised from the leaves to the root of the tree (Figure 3).
3. finally, starting from the root, all the nodes are considered in pairs and merged if the resulting transducer does not contradict the data of the training set (Figures 4 and 5). The order of these merging attempts is usually indicated by a heuristic function. It is possible to push output sequences towards the sheets to allow mergers. When no more merging is possible, the algorithm ends."
French,article-10-13-fr,"Pour se focaliser sur les termes qui sont morphologiquement proches, la similarité formelle de chaque paire a été évaluée à l’aide d’une distance d’édition normalisée par la longueur des mots. Les paires sont classées dans une liste selon ce score en ordre décroissant de similarité :"
English,article-10-13-eng,"In order to focus on morphologically related pairs, a formal similarity was computed for each pair through the string edit distance. Pairs were then ordered in a list according to their scores in descending order."
TEng,article-10-13-teng,"To focus on terms that are morphologically close, the formal similarity of each pair was assessed using an editing distance normalized by the length of the words. The pairs are ranked in a list according to this score in descending order of similarity:"
French,article-10-15-fr,"Pour chacune des expériences, nous testons les sens de traductions français vers anglais et l’inverse, avec différentes tailles de jeux d’entraînement. Les jeux de test comportent 2000 paires (bien entendu différentes des paires d’entraînement) et les processus d’inférence et de validation sont répétés dix fois et les résultats moyennés. L’expérience 2 est une évaluation dans le pire des cas puisque les transducteurs inférés sont testés sur des paires qui peuvent n’être pas morphologiquement apparentées."
English,article-10-15-eng,"For each experiment, we test our approach for the translation of terms from French to English and from English to French. The inference process is repeated 10 times: the initial set is divided in 10 folds and ostia uses 9 of these folds; the tenth fold is different each time. Thus, ten transducers are inferred; this allows us to average their results (see Section 4). Each test set comprises 2000 pairs (of course different from the training pairs)."
TEng,article-10-15-teng,"For each of the experiments, we test the meanings of French to English translations and vice versa, with different sizes of training games. The test sets consist of 2000 pairs (of course different from the training pairs) and the inference and validation processes are repeated ten times and the results averaged. Experiment 2 is a worst-case assessment since the inferred transducers are tested on pairs which may not be morphologically related."
French,article-10-2-fr,"Introduction
Dans le domaine biomédical, l’évolution rapide des connaissances et la prédominance de l’anglais comme langue de communication rendent cruciales les problématiques de production et de gestion de ressources terminologiques multilingues. Dans ce cadre, la traduction de terminologies existantes, dont fait l’objet cet article, revêt une grande importance."
English,article-10-2-eng,"Introduction
In the biomedical domain, the international research framework and fast knowledge update make producing, managing and updating multilingual resources an important issue. Within this context, this paper presents and evaluates an original method to automatically translate a large class of biomedical simple terms"
TEng,article-10-2-teng,"Introduction
In the biomedical field, the rapid evolution of knowledge and the predominance of English as a language of communication make crucial the problems of production and management of multilingual terminological resources. In this context, the translation of existing terminologies, which is the subject of this article, is of great importance."
French,article-10-7-fr,"Un transducteur séquentiel est un transducteur dans lequel tous les états sont finals, et où il est impossible d’avoir deux arcs sortant d’un même état ayant le même symbole d’entrée. Cette dernière propriété est celle qui assure le déterminisme des traductions issues de ces transducteurs. Enfin, un transducteur sous-séquentiel est un transducteur séquentiel dans lequel à chaque état est associée une séquence de sortie. Celle-ci est produite lorsque la séquence d’entrée se termine sur l’état ; c’est ce qui rend les transducteurs sous-séquentiels relativement expressifs."
English,article-10-7-eng,"A sub-sequential transducer is a deterministic transducer (two edges with the same input symbol cannot emerge from the same state) in which all the states are final, and having an output string associated to each state. This string is produced when the input sequence to be accepted by the transducer ends on the state it is associated with."
TEng,article-10-7-teng,"A sequential transducer is a transducer in which all the states are final, and where it is impossible to have two arcs coming out of the same state having the same input symbol. This last property is that which ensures the determinism of the translations coming from these transducers. Finally, a sub-sequential transducer is a sequential transducer in which an output sequence is associated with each state. This is produced when the entry sequence ends on the state; this is what makes sub-sequential transducers relatively expressive."
French,article-11-1-fr,"Dans cet article, nous nous intéressons à l’identification automatique des paires de documents parallèles contenues dans un corpus bilingue. Nous montrons que cette tâche peut être accomplie avec précision en utilisant un ensemble restreint d’invariants lexicaux. Nous évaluons également notre approche sur une tâche de traduction automatique et montrons qu’elle obtient des résultats supérieures à un système de référence faisant usage d’un lexique bilingue."
English,article-11-1-eng,In this study we address the problem of automatically identifying the pairs of texts that are translation of each other in a set of documents. We show that it is possible to automatically build particularly efficient content-based methods that make use of very little lexical knowledge. We also evaluate our approach toward a front-end translation task and demonstrate that our parallel text classifier yields better performances than another approach based on a rich lexicon.
TEng,article-11-1-teng,"In this article, we are interested in the automatic identification of pairs of parallel documents contained in a bilingual corpus. We show that this task can be accomplished with precision using a small set of lexical invariants. We also evaluate our approach on a machine translation task and show that it achieves results superior to a reference system using a bilingual lexicon."
French,article-11-13-fr,"Résultats
Nous avons comparé les performances de notre moteur de TA lorsqu’il est entraîné sur quatre corpus parallèles différents. Contrairement à nos expériences sur EUROPARL, la mesure de cosinus et la distance d’édition ont des performances comparables. Cela pourrait s’expliquer par la plus petite taille des documents de PAHO (rendant l’ordre des caractéristiques moins important) et par l’étape de suppression des paires partageant un document."
English,article-11-13-eng,"Results
We compared the performance of our translation engine when trained on four different bitexts. Contrary to our former experiment, cosine and edit performed similarly well. This could be explained by the shorter length of the documents and by the filtering step applied on the parallel pairs, which seems to eliminate untrusted pairs. Inspection of their bitexts showed that they shared only 229 document pairs, so we trained another translation engine on the union of those bitexts. The scores of all the translation engines are reported in Table 2."
TEng,article-11-13-teng,"Results
We compared the performance of our AT engine when it is trained on four different parallel corpora. Contrary to our experiences on EUROPARL, the cosine measurement and the editing distance have comparable performances. This could be explained by the smaller size of PAHO documents (making the order of characteristics less important) and by the step of removing pairs sharing a document."
French,article-11-2-fr,"Introduction
De nos jours, les corpus de documents parallèles (ensemble de documents exprimant le même contenu dans le même ordre) jouent un rôle crucial dans les applications multilingues de traitement automatique des langues. Aligné au niveau des phrases, une tâche pouvant être accomplie avec fiabilité, un corpus parallèle s’avère très utile aux concordanciers bilingues et est la pierre angulaire de la plupart des systèmes commerciaux de mémoire de traduction."
English,article-11-2-eng,"Introduction
Parallel corpora are currently playing a crucial role in multilingual natural language processing applications. Aligned at the sentence level, a task that has been shown to be fairly easy, a parallel corpus turns out to be already very useful for bilingual concordancers and is the cornerstone of most of the commercial translation memory systems that have been and still are popular among professional translators."
TEng,article-11-2-teng,"Introduction
Nowadays, the corpus of parallel documents (set of documents expressing the same content in the same order) play a crucial role in multilingual applications of automatic language processing. Aligned at the sentence level, a task that can be reliably accomplished, a parallel corpus is very useful for bilingual concorders and is the cornerstone of most commercial translation memory systems."
French,article-11-5-fr,Nous évaluons également notre approche à travers une tâche de traduction automatique et mesurons des performances supérieures à celles d’une approche faisant usage d’un lexique bilingue riche (section 4). Nous discutons en section 5 de travaux connexes et présentons en section 6 nos conclusions.
English,article-11-5-eng,"In sections 3 and 4 we evaluate our approaches on two tasks: a controlled one on a part of the europarl corpus, as well as a real task we faced when developing an English- Spanish concordancer.We then show that some of the approaches we investigated are very effective at identifying parallel texts and that their use for seeding a translation engine is also fruitful."
TEng,article-11-5-teng,We also evaluate our approach through an automatic translation task and measure performance superior to that of an approach using a rich bilingual lexicon (section 4). We discuss in section 5 related work and present in section 6 our conclusions.
French,article-11-9-fr,"Expérience contrôlée EUROPARL est un corpus parallèle tiré de la transcription des débats parlementaires européens s’étant tenus entre avril 1996 et septembre 2003 (Koehn, 2002). Les débats parlementaires européens sont traduits en onze langues, mais nous nous sommes concentrés sur les traductions anglaises et espagnoles. Notre corpus était composé de 487 textes anglais et de 487 textes espagnols ayant en moyenne environ 2800 phrases chacun."
English,article-11-9-eng,"Controlled Task
europarl is a large corpus of bitexts drawn from the European Parliament between April 1996 and September 2003 [15]. It includes versions of the documents in 11 languages, but we focus in this study on the English-Spanish bitext. Our test corpus contains 487 English documents (therefore 487 Spanish ones), thus summing to 237,169 potential pairs of documents. Each document contains an average of about 2,800 sentences."
TEng,article-11-9-teng,"Controlled experience EUROPARL is a parallel corpus drawn from the transcription of European parliamentary debates held between April 1996 and September 2003 (Koehn, 2002). European parliamentary debates are translated into eleven languages, but we have focused on English and Spanish translations. Our corpus was made up of 487 English texts and 487 Spanish texts with an average of approximately 2800 sentences each."
French,article-13-0-fr,Résumé PrepLex est un lexique des prépositions du français. Il contient les informations utiles à des systèmes d’analyse syntaxique. Il a été construit en comparant puis fusionnant différentes sources d’informations lexicales disponibles. Ce lexique met également en évidence les prépositions ou classes de prépositions qui apparaissent dans la définition des cadres de sous-catégorisation des ressources lexicales qui décrivent la valence des verbes.
English,article-13-0-eng,Abstract PrepLex is a lexicon of French prepositions which provides all the information needed for parsing. It was built by comparing and merging several authoritative lexical sources. This lexicon also shows the prepositions or classes of prepositions that appear in verbs subcategorization frames.
TEng,article-13-0-teng,Summary PrepLex is a lexicon of French prepositions. It contains information useful for parsing systems. It was built by comparing and then merging different sources of lexical information available. This lexicon also highlights the prepositions or classes of prepositions that appear in the definition of lexical resource sub-categorization frameworks that describe the valence of verbs.
French,article-13-10-fr,"Notre travail est destiné à fournir un lexique utilisable par un analyseur syntaxique. Nous nous sommes restreints aux aspects purement syntaxiques et à quelques éléments sémantiques comme la définition des ensembles de prépositions ayant un aspect sémantique commun (comme LOC). Le lexique produit est diffusé sous licence libre et a vocation à être intégré dans des ressources plus larges, existantes ou à venir."
English,article-13-10-eng,"Our work aims at providing the community with a lexicon that can be directly used by a parser. We focused on syntactic aspects and extended the work to some semantic elements, like semantically linked sets of prepositions (as LOC). The generated lexicon is freely available and is expected to be integrated into larger resources for French, whether existing or under development."
TEng,article-13-10-teng,"Our work is intended to provide a lexicon usable by a syntactic analyzer. We have limited ourselves to purely syntactic aspects and to a few semantic elements such as the definition of sets of prepositions with a common semantic aspect (like LOC). The lexicon produced is distributed under a free license and is intended to be integrated into larger resources, existing or future."
French,article-13-15-fr,"Nous avons donc effectué, dans un premier temps, un travail d’inventaire des prépositions présentes dans un certain nombre de ressources, des lexiques et/ou dictionnaires d’une part, pour la liste générale, des lexiques syntaxiques d’autre part, pour la liste de prépositions argumentales. Deux ressources se classent cependant dans les deux catégories, le Lefff et le dictionnaire UNL :"
English,article-13-15-eng,"We thus collected, as a first step, prepositions from a certain number of resources, lexicons and dictionaries for the garden-variety list, and syntactic lexicons for the argument prepositions list. Two resources belong to both categories, Lefff and French- UNL dictionary:
"
TEng,article-13-15-teng,"We therefore carried out, as a first step, an inventory of the prepositions present in a certain number of resources, lexicons and / or dictionaries on the one hand, for the general list, syntactic lexicons on the other hand, for the list of argument prepositions. However, two resources fall into two categories, the Lefff and the UNL dictionary:"
French,article-13-17-fr,"UNL (Universal Networking Language (Sérasset & Boitet, 2000)), est un dictionnaire du français vers un anglais désambiguïsé conçu pour la traduction automatique, qui comprend des informations syntaxiques dans sa partie française. UNL n’a qu’une couverture assez faible (moins de 27 000 lemmes), mais il propose dans sa partie anglaise des informations de type sémantique que nous envisageons d’utiliser par la suite. UNL contient 48 prépositions simples dont 10 apparaissant dans les cadres de sous-catégorisation des verbes."
English,article-13-17-eng,"UNL (Universal Networking Language (S´erasset and Boitet, 2000)), is a French to disambiguated English dictionary for machine translation, which contains syntactic information in its French part (see table 1 for a UNL example entry). UNL has limited coverage (less than 27,000 lemmas), but it provides, in the English part, semantic information that we will consider using in the near future. UNL contains 48 simple prepositions, among which 12 appear in verb subcategorization frames."
TEng,article-13-17-teng,"UNL (Universal Networking Language (Sérasset & Boitet, 2000)), is a disambiguated French to English dictionary designed for machine translation, which includes syntactic information in its French part. UNL has only fairly weak coverage (less than 27,000 lemmas), but it offers semantic information in its English part which we plan to use later. UNL contains 48 simple prepositions, 10 of which appear in the sub-categorization of verbs."
French,article-17-0-fr,"Dans le cadre du projet Papillon qui vise à la construction de bases lexicales multilingues par acceptions, nous avons défini des stratégies pour peupler un dictionnaire pivot de liens interlingues à partir d’une base vectorielle monolingue. Il peut y avoir un nombre important de sens par entrée et donc l’identification des acceptions correspondantes peut être erronée. Nous améliorons l’intégrité de la base d’acception grâce à des agents experts dans les fonctions lexicales comme la synonymie, l’antonymie, l’hypéronymie ou l’holonymie. Ces agents sont capable de calculer la pertinence d’une relation sémantique entre deux acceptions par les diverses informations lexicales récoltées et les vecteurs conceptuels."
English,article-17-0-eng,"In the framework of the Papillon project, we have defined strategies for populating a pivot dictionnary of interlingual links from monolingual vectorial bases. There are quite a number of acceptions per entry thus, the proper identification may be quite troublesome and some added clues beside acception links may be useful. We improve the integrity of the acception base through well known semantic relations like synonymy, antonymy, hyperonymy and holonymy relying on lexical functions agents. These semantic relation agents can compute the pertinence of a semantic relation between two acceptions thanks to various lexical informations and conceptual vectors."
TEng,article-17-0-teng,"Within the framework of the Papillon project which aims at building multilingual lexical bases by acceptances, we defined strategies to populate a pivot dictionary of interlingual links from a monolingual vector base. There can be a significant number of meanings per entry and therefore the identification of the corresponding meanings may be incorrect. We improve the integrity of the acceptance basis thanks to agents expert in lexical functions such as synonymy, anonymity, hyperonymy or holonymy. These agents are capable of calculating the relevance of a semantic relationship between two meanings by the various lexical information collected and the conceptual vectors."
French,article-17-14-fr,"Liens sémantiques
Les relations sémantiques entre items lexicaux structurent le lexique sur le plan paradigmatique. Ces relations sont de deux types : les relations hiérarchiques (hyponymie/hypéronymie, méronymie/holonymie) et les relations d’équivalence/opposition (synonymie, antonymie). Elles sont souvent décrites comme des relations booléennes, i.e, elles existent entre deux items ou non (Polguère, 2001). Dans le cas des acceptions, monosémiques par définitions, les relations hiérarchiques sont transitives et la synonymie peut être considérée comme une équivalence."
English,article-17-14-eng,"Semantic Links and Integrity Constraints
The semantic relations between lexical units help structuring the lexicon on the paradigmatic plane. These well known relations are often described in two types: the hierarchical relations (hyponymy, hyperonymy, meronymy and holonymy) and the equivalence/opposition relations (synonymy, antonymy). In some linguistic research ([Polguere, 2001 ], [Lehmann et Martin-Berthet, 98]) they are defined as boolean relations i.e. they either hold between two words or they don't. For acceptions, which are monosemic, hierarchical relations are transitive and synonymy is considered as an equivalence."
TEng,article-17-14-teng,"Semantic links
The semantic relationships between lexical items structure the lexicon on a paradigmatic level. These relationships are of two types: hierarchical relationships (hyponymy / hyperonymy, meronymy / holonymy) and equivalence / opposition relationships (synonymy, antonymy). They are often described as Boolean relationships, i.e., they exist between two items or not (Polguère, 2001). In the case of acceptances, monosemic by definition, hierarchical relationships are transitive and synonymy can be considered as an equivalence."
French,article-17-18-fr,"Création et suppression de liens
Nous voulons ajouter des liens sémantiques à la base d’acceptions afin, non seulement de constituer un réseau sémantique mais également pour pouvoir vérifier son intégrité. Les agents capables d’évaluer une relation sémantique valuée peuvent créer des liens sémantiques valués si la valuation est supérieure à un seuil s. Par exemple, si un agent expert d’antonymie évalue l’antonymie entre ,froidet ,chaud- à une valeur n supérieure à s, il construit un lien sémantique entre les deux acceptions valué à n."
English,article-17-18-eng,"Links Creation and Deletion
We want to add semantics links to the acception base in order to assess the integrity of the base. Agents which can evaluate a given semantic relation can create valued semantics links if the valuation result is above a threshold th. For exemple, if one antonymy agent evaluates antonymy between ,cold- and ,hot- at a value above th, it builds a semantic link between the two acceptions valued at th. This threshold is not fixed in advance, and it evolves according to the number of links already built."
TEng,article-17-18-teng,"Creation and removal of links
We want to add semantic links to the acceptance base in order not only to constitute a semantic network but also to be able to verify its integrity. Agents capable of evaluating a valued semantic relationship can create valued semantic links if the valuation is greater than a threshold s. For example, if an expert anonymity agent evaluates the anonymity between, cold and warm - at a value n greater than s, he builds a semantic link between the two meanings valued at n."
French,article-17-19-fr,Le seuil n’est pas fixé en avance et il évolue constament en fonction du nombre de liens déjà construits. Le système apprend en fonction des nouvelles données (nouveaux dictionnaires monolingues ou bilingues) ou en révisant les anciennes données et donc les agents doivent régulièrement recalculer les relations et si la condition pour préserver le lien (être supérieur à s) n’est plus respectée alors le lien est détruit.
English,article-17-19-eng,"Thus, this value will evolve during time. The system learns from new data (new monolingual or bilinguals dictionnaries) or by revising old data so agents can compute again a relation and if the condition to preserve the link, to be above th, is not verified the link is deleted."
TEng,article-17-19-teng,The threshold is not set in advance and it is constantly changing depending on the number of links already built. The system learns according to the new data (new monolingual or bilingual dictionaries) or by revising the old data and therefore the agents must regularly recalculate the relationships and if the condition to preserve the link (be greater than s) is no longer met the link is destroyed.
French,article-17-2-fr,"Introduction
La recherche en représentation de sens est un important problème qui a été abordé selon plusieurs approches. Notre équipe travaille actuellement sur l’analyse thématique de textes et la désambiguïsation lexicale (Lafourcade, 2001). Nous construisons un système capable d’apprentissage automatique basé sur les vecteurs conceptuels. Les vecteurs contiennent les idées associées aux mots ou expressions. Le système d’apprentissage construit ou révise automatiquement les vecteurs conceptuels à partir de définitions en langage naturel contenues dans les dictionnaires à usage humain."
English,article-17-2-eng,"Introduction
Research in meaning representation in NLP is an important problem still addressed through several approaches. The NLP team from LIRMM currently works on thematic text analysis and lexical disambiguation [Lafourcade, 2001]. To this purpose, we built a system, with automated learning capabilities, based on conceptual vectors for meaning representation. Vectors are supposed to encode `ideas' associated to words or expressions. The conceptual vectors learning system automatically defines or revises its vectors from definitions in natural language contained in electronic dictionaries for human usage."
TEng,article-17-2-teng,"Introduction
Research into the representation of meaning is an important problem which has been approached according to several approaches. Our team is currently working on thematic text analysis and lexical disambiguation (Lafourcade, 2001). We are building a system capable of machine learning based on conceptual vectors. The vectors contain the ideas associated with the words or expressions. The learning system automatically constructs or revises conceptual vectors from natural language definitions contained in dictionaries for human use."