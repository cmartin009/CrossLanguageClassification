Label,ID,Text
French,article-0-0-fr,"Résumé 
Dans ce papier, nous proposons une méthode pour résumer et interroger des logs de requêtes OLAP. L’idée de base est qu’une requête résume une autre requête et qu’un log, qui est une séquence de requêtes, résume un autre log. Notre cadre formel est composé d’une algèbre simple destinée à résumer des requêtes OLAP, et d’une mesure évaluant la qualité du résumé obtenu. Nous proposons également plusieurs stratégies pour calculer automatiquement des résumés de logs de bonne qualité, et nous montrons comment des propriétés simples sur les résumés peuvent être utilisées pour interroger un log efficacement. Des tests sur des logs de requêtes MDX ont montré l’intérêt de notre approche."
English,article-0-0-eng,"Abstract In this paper, we propose a method for summarizing OLAP query logs. The basic idea is that a query summarizes another query and that a log, which is a sequence of queries, summarizes another log. Our framework allows to declaratively specify a summary, and includes a measure to assess the quality of the summaries.We also propose several strategies for automatically computing a good quality summary of a query log, and we show how some simple properties on the summaries can be used to query the log efficiently. Tests on MDX query logs showed the usefulness of our approach."
TEng,article-0-0-teng,"Summary
In this paper, we propose a method for summarizing and interrogating OLAP request logs. The basic idea is that a request summarizes another request and that a log, which is a sequence of requests, summarizes another log. Our formal framework is composed of a simple algebra intended to summarize OLAP queries, and of a measure evaluating the quality of the summary obtained. We also offer several strategies for automatically calculating good quality log summaries, and we show how simple properties on summaries can be used to query a log efficiently. Tests on MDX request logs have shown the interest of our approach."
French,article-0-14-fr,"Intuition
Nous cherchons à évaluer si une requête (resp., un log), qui correspond à un ensemble de références (resp., requêtes), résume fidèlement une autre requête (resp., un autre log). Les opérateurs de QSL résument en conservant, ajoutant ou retirant des références aux requêtes à résumer pour constituer le résumé. Une manière d’évaluer cette fidélité est de mesurer la proportion de ce qui est ajouté ou enlevé par l’opérateur. Nous nous sommes donc tournés vers les mesures de rappel et précision classiques en recherche d’information."
English,article-0-14-eng,"Intuition
The measure should assess to which extends a query (respectively, a log), which is a set of references (respectively, of queries), is a faithful summary of another query (respectively, another log). The operators of QSL define summaries by adding or removing references to their operands. For instance the lca operator summarizes by adding references containing ancestors. The measure should thus assess the proportion of what is added or removed to define the summary. This is achieved by adapting the classical notion of precision and recall."
TEng,article-0-14-teng,"Intuition
We seek to assess whether a request (resp., A log), which corresponds to a set of references (resp., Requests), faithfully summarizes another request (resp., Another log). The operators of QSL summarize by preserving, adding or removing references to the queries to be summarized to constitute the summary. One way to assess this fidelity is to measure the proportion of what is added or removed by the operator. We therefore turned to conventional recall and precision measurements in search of information."
French,article-0-16-fr,"Ainsi, nous proposons d’étendre rappel et précision par la prise en compte d’une relation de couverture. Pour être le plus général possible, nous choisissons la relation de couverture définie sur les références puisque tant les requêtes que les logs peuvent être assimilés à des ensembles de références. Ainsi définie de manière générale, cette mesure présente l’intérêt de pouvoir être appliquée sur deux requêtes ou deux logs, ou n’importe quel couple d’ensembles de références."
English,article-0-16-eng,"We propose to extend recall and precision by taking into account a cover relation between the elements of the two sets, the summary and the summarized. In this article we use the cover relation defined over references since both queries and logs can be seen as sets of references, and thus the quality measure can be used on queries or on logs, or on any sets of references. Note that the definition of the measure is even more general in the sense that it does not rely on a particular cover relation. We now formalize these notions."
TEng,article-0-16-teng,"Thus, we propose to extend recall and precision by taking into account a hedging relationship. To be as general as possible, we choose the coverage relationship defined on the references since both the queries and the logs can be assimilated to sets of references. Thus defined in general, this measure has the advantage of being able to be applied to two requests or two logs, or any couple of sets of references."
French,article-0-2-fr,"Dans cet article, nous développons les travaux entrepris sur le résumé de log de requêtes OLAP (Aligon et al., 2010a), où nous proposions un langage de manipulation de requêtes (appelé QSL) pour résumer des requêtes OLAP, une mesure de la qualité d’un résumé et un algorithme glouton de construction automatique de résumés de log de requêtes utilisant QSL. Sur cette base, nous proposons un nouvel algorithme de calcul de résumés et deux sous-langages de QSL dont nous étudions les propriétés. Nous indiquons comment ces propriétés sont utilisées pour interroger un log. Finalement, nous présentons l’implémentation de notre cadre et quelques tests illustrant son efficacité."
English,article-0-2-eng,"In this article we present and develop the work initiated in [1, 2]. In these papers, we proposed a framework for summarizing an OLAP query log, and we studied basic properties of the framework for helping the user to query the log. The present article provides a detailed presentation of the framework and introduces its implementation as a system for summarizing and querying log files. To this end, we extend the search facilities introduced in [2] to obtain a declarative language with which complex queries over a log file can be expressed."
TEng,article-0-2-teng,"In this article, we develop the work undertaken on the OLAP request log summary (Aligon et al., 2010a), where we proposed a request manipulation language (called QSL) to summarize OLAP requests, a measure of quality. of a summary and a greedy algorithm of automatic construction of summaries of request log using QSL. On this basis, we propose a new algorithm for calculating summaries and two QSL sublanguages whose properties we are studying. We indicate how these properties are used to query a log. Finally, we present the implementation of our framework and some tests illustrating its effectiveness."
French,article-0-4-fr,"Dans la section 5, nous présentons notre nouvel algorithme de résumé et introduisons quelques propriétés des résumés obtenus à l’aide des sous-langages de QSL. La section 6 présente l’exploitation des propriétés de notre cadre pour interroger des logs. La section 7 présente notre implémentation et les tests conduits. Nous discutons des travaux connexes et concluons dans la section 8."
English,article-0-4-eng,"In Section 5, we present the algorithm that automatically constructs summaries based on QSL and the quality measure. We also introduce the properties of the summaries constructed with sub-languages of QSL. Section 6 presents the language for querying logs, and describes how properties of the framework can be used to ensure efficient searches. Section 7 describes the implementation of the framework and the experiments conducted to evaluate its effectiveness. Section 8 discusses related work. We conclude and draw perspectives in Section 9."
TEng,article-0-4-teng,"In section 5, we present our new summary algorithm and introduce some properties of summaries obtained using QSL sublanguages. Section 6 presents the exploitation of the properties of our framework to query logs. Section 7 presents our implementation and the tests conducted. We discuss related work and conclude in section 8."
French,article-0-7-fr,"Tout d’abord notons que plusieurs manières de résumer sont pertinentes. Par exemple, en résumant le log par la seule requête mentionnant les membres les plus fréquemment interrogés, ce qui intéressera un administrateur souhaitant savoir quels indexes positionner. Dans notre exemple une telle requête demanderait les ventes de Coke dans les régions North et South pour July 2008 (la requête q2). Une autre manière de résumer permettrait à un utilisateur de savoir grossièrement ce qui a été fait sur le cube, sous la forme d’une requête interrogeant les ventes de soda en France en 2008. Si l’utilisateur est intéressé par plus de détails, il pourra ensuite interroger le log pour trouver des requêtes précises. Un autre intérêt d’un résumé, plus compact qu’un log, est donc de pouvoir être utilisé en amont du log pour rendre l’interrogation plus efficace."
English,article-0-7-eng,"Assume we want to summarize these queries by another query. Various solutions are possible. First, we can summarize the queries by retaining for each dimension the most frequent members. This could be of interest for a DBA who would like to know what indices to store. In that case, the resulting query would ask for sales of Coke in regions North or South during July 2008 (i.e., query q2). A second alternative would be to summarize the queries with another query having for each dimension the members that cover all members present in the initial queries. For example, note that Pepsi, Coke and Orangina are sodas, cities Paris and Marseille and regions North and South are in France and all three queries concern year 2008. The query summarizing the log L would then ask for the sales of Soda in France in 2008. The user interested in more details on the query could then query the log to find the queries that were indeed launched."
TEng,article-0-7-teng,"First, note that several ways of summarizing are relevant. For example, by summarizing the log by the only query mentioning the members most frequently questioned, which will interest an administrator wishing to know which indexes to position. In our example, such a query would request sales of Coke in the North and South regions for July 2008 (query q2). Another way of summarizing would allow a user to know roughly what was done on the cube, in the form of a query querying sales of soda in France in 2008. If the user is interested in more details, he can then query the log to find specific requests. Another advantage of a summary, more compact than a log, is therefore that it can be used upstream of the log to make the query more efficient."
French,article-1-12-fr,"Une brève description des données est reportée en table 1. L’expérience “train-test” consiste à diviser la base de données en deux sous-ensembles en respectant la distribution des classes. La première sert à l’apprentissage (i.e. l’extraction de règles selon des seuils donnés de fréquence, confiance et taux d’accroissement), la deuxième sert à évaluer l’évolution des valeurs des mesures (en test). Nous calculons et comparons aussi les valeurs de level des règles extraites en apprentissage et en test."
English,article-1-12-eng,"The train-test experiments consist in dividing a data set in two (almost) equal class-stratified parts. One part is for training and mining frequent-confident (or emerging) rules, the other part is for evaluating the evolution of confidence and growth rate values on the test set. Since we do not provide an extractor of MODL rules in this preliminary work, we compute the value of our MODL criterion for the extracted confident (or emerging) rules on the training and test set for comparison."
TEng,article-1-12-teng,"A brief description of the data is given in table 1. The “train-test” experiment consists in dividing the database into two subsets while respecting the distribution of the classes. The first is used for learning (i.e. the extraction of rules according to given thresholds of frequency, confidence and rate of increase), the second is used to evaluate the evolution of the values of the measures (in test). We also compute and compare the level values of the rules extracted in learning and testing."
French,article-1-14-fr,"Données originales. Dans les graphiques de la figure 2, nous reportons l’évolution train-test des valeurs de taux d’acroissement (GR) pour chaque base de données. Nous comparons aussi les valeurs de level des règles extraites. Nous remarquons que GR est généralement instable : en effet, une règle à fort taux d’accroissement en apprentissage peut avoir un faible GR en test (voir les points éloignés de la droite identité) – ce qui confirme notre hypothèse que le taux d’accroissement (comme la confiance) ne capturent pas la notion de robustesse. Au contraire les valeurs de level sont très stables (points proche de l’identité)."
English,article-1-14-eng,"Original data sets. In figures 2 and 3, we report scatter plots for the study of the evolution (from train set to test set) of confidence values of extracted rules. We also compare the values of the MODL criterion. As expected, for all data sets, we observe that confidence is unstable from train to test: indeed, a highly confident rule in train may have low confidence in test (see the points far from the identity line). Conversely, the MODL level values of extracted rules are rather stable in the traintest experiments (see the points close to the identity line)."
TEng,article-1-14-teng,"Original data. In the graphs in Figure 2, we report the train-test evolution of the growth rate (GR) values for each database. We also compare the level values of the extracted rules. We note that GR is generally unstable: indeed, a rule with a high rate of increase in learning can have a low GR in test (see the points far from the right identity) - which confirms our hypothesis that the rate of increase (like trust) do not capture the notion of robustness. On the contrary, the level values are very stable (points close to identity)."
French,article-1-16-fr,"Données bruitées. Afin de simuler la présence de bruit de classe dans les données breast-w, nous ajoutons de manière uniforme du bruit à l’attribut classe (changement de classe) en utilisant la fonction AddNoise de WEKA (Witten et Frank, 2005). Nous utilisons deux niveaux de bruit : moyen (20%) et fort (50%). Nous renouvelons l’expérience train-test sur chaque version artificiellement bruitée. Les résultats sont reportés en figure 5. A chaque niveau de bruit, les extracteurs classiques réussissent à extraire des motifs “potentiellement” intéressants – notons tout de même que beaucoup moins de règles sont extraites des contextes fortement bruités. Cependant, l’expérience train-test montre encore une fois l’instabilité des mesures classiques et cette instabilité est d’autant plus grande lorsque le contexte est très bruité (50%)."
English,article-1-16-eng,"Noisy data. In order to simulate the presence of class noise in breast-w data, we uniformly add noise to the class attribute (class change) using WEKA's AddNoise function (Witten and Frank, 2005). We use two noise levels: medium (20%) and loud (50%). We renew the train-test experience on each artificially noised version. The results are shown in Figure 5. At each noise level, the classic extractors manage to extract “potentially” interesting patterns - note however that far fewer rules are extracted from highly noisy contexts. However, the train-test experience once again shows the instability of conventional measurements and this instability is even greater when the context is very noisy (50%)."
TEng,article-1-16-teng,"Noisy data. In order to simulate the presence of class noise in breast-w data, we uniformly add noise to the class attribute (class change) using WEKA's AddNoise function (Witten and Frank, 2005). We use two noise levels: medium (20%) and loud (50%). We renew the train-test experience on each artificially noised version. The results are shown in Figure 5. At each noise level, the classic extractors manage to extract “potentially” interesting patterns - note however that far fewer rules are extracted from highly noisy contexts. However, the train-test experience once again shows the instability of conventional measurements and this instability is even greater when the context is very noisy (50%)."
French,article-1-2-fr,"Le paramétrage. Le seuillage de la mesure d’intérêt utilisée est une étape cruciale et pour autant non-triviale. Le dilemme est bien connu : un seuil de fréquence minimum élevé génère moins de règles mais aussi un faible taux de couverture des données et souvent moins de pouvoir discriminant pour les classes du problème. D’un autre côté, un seuil de fréquence très bas génère un grand nombre de règles parmi lesquelles certaines (de faible fréquence) peuvent être erronées."
English,article-1-2-eng,"The Curse of Parameters. The choice of parameter values is crucial but not trivial. The dilemma is well-known: a high frequency threshold may lead to less rules, but also lesser coverage rate and less discriminating power. A low frequency threshold may lead to a huge amount of rules, among which some rules (with low frequency) may be spurious."
TEng,article-1-2-teng,"Setting. The thresholding of the measure of interest used is a crucial and non-trivial step. The dilemma is well known: a high minimum frequency threshold generates fewer rules but also a low data coverage rate and often less discriminating power for the classes of the problem. On the other hand, a very low frequency threshold generates a large number of rules, some of which (of low frequency) may be wrong."
French,article-1-21-fr,"A propos du principe MDL. Siebes et al. (2006) propose une approche d’extraction de motifs basée sur le principe MDL. Les auteurs cherchent à extraire les itemsets qui fournissent une bonne compression des données. Le lien entre probabilités et longueurs de codage permet aux auteurs de réécrire la longueur de codage d’un itemset I en -log(P(I)). Ainsi, les “meilleurs” itemsets ont un codage plus court et compressent mieux les données. Dans (van Leeuwen et al., 2006), une extension pour la classification supervisée est proposée. Les deux principales différences avec l’approche MODL sont les suivantes : (i), l’utilisation de l’apriori hiérarchique MODL implique un codage différent ; (ii), Siebes et al. (2006) cherchent un ensemble de motifs qui compressent les données alors qu’ici notre critère est défini pour une règle."
English,article-1-21-eng,"About MDL. In [Siebes et al., 2006], the authors develop a MDL-based pattern mining approach. The authors look for itemsets that provides a good compression of the data. The link between probability and codes allow them to rewrite the code length of an item set I as -log(P(I)). Thus, the best item sets have shortest codes. In [van Leeuwen et al., 2006], an extension for classification purpose is suggested. The two main differences with the MODL approach are : (i) the use of the MODL hierarchical prior implies a different way of coding information ; (ii) in [van Leeuwen et al., 2006], authors look for a set of patterns to compress the data whereas our MODL criterion is defined for one rule."
TEng,article-1-21-teng,"About the MDL principle. Siebes et al. (2006) proposes an approach to extracting patterns based on the MDL principle. The authors seek to extract the itemsets which provide good data compression. The link between probabilities and coding lengths allows authors to rewrite the coding length of an itemset I in -log (P (I)). Thus, the “best” itemsets have a shorter coding and compress the data better. In (van Leeuwen et al., 2006), an extension for supervised classification is proposed. The two main differences with the MODL approach are as follows: (i), the use of the MODL hierarchical overview implies different coding; (ii), Siebes et al. (2006) look for a set of patterns that compress the data while here our criterion is defined for a rule."
French,article-1-3-fr,"Le même dilemme persiste pour le seuillage de mesures d’intérêt telles que la confiance (i.e. une estimation de la probabilité P(c | X)) ou le taux d’accroissement (qui permet d’identifier les motifs émergents, i.e., qui sont fréquents dans une classe de données et infréquents dans le reste de la base (Dong et Li, 1999)). En effet, des seuils élevés de confiance (ou de taux d’accroissement) génèrent un petit nombre de règles de classification presque pures qui sont rares (voire erronées si combinées avec un seuil de fréquence bas) alors que des seuils bas génèrent beaucoup de règles avec un intérêt limité. Ainsi, trouver un compromis entre seuil de fréquence et mesure d’intérêt n’est pas trivial."
English,article-1-3-eng,"The same dilemma stands when thresholding interestingness measures like confidence (i.e. an estimation of the probability P(c | X)) or growth rate (which highlights the so-called emerging patterns, i.e. those patterns that frequent in a class of the data set and barely infrequent in the rest of the data [Dong and Li, 1999]): indeed, high confidence (or growth rate) threshold values lead to strong (pure) class association rules which may be rare in real-world data or even wrong when combined with a low frequency threshold whereas “low” thresholds generate a lot of rules with limited interest. Thus, finding a trade-off between frequency and interestingness measure values is not trivial."
TEng,article-1-3-teng,"The same dilemma persists for the thresholding of measures of interest such as confidence (ie an estimate of the probability P (c | X)) or the rate of increase (which makes it possible to identify the emerging patterns, ie, which are frequent in a data class and infrequent in the rest of the database (Dong and Li, 1999)). Indeed, high confidence (or rate of increase) thresholds generate a small number of almost pure classification rules which are rare (even erroneous if combined with a low frequency threshold) while low thresholds generate a lot of rules with limited interest. Thus, finding a compromise between frequency threshold and interest measure is not trivial."
French,article-1-4-fr,"L’instabilité des mesures d’intérêt. Bien que des sous-ensembles de règles permettent de bonnes prédictions, il est facile de montrer que des règles à forte confiance ou émergentes ne sont pas individuellement robustes. Dans la figure 1, nous comparons les valeurs de confiance (resp. de taux d’accroissement et de lift (Brin et al., 1997)) en apprentissage et en test de règles extraites de la base UCI breast-w (Asuncion et Newman, 2007)."
English,article-1-4-eng,"Instability of interestingness measures. Even if subsets of extracted rules have
 shown to be quite effective for predictions, it can be easily shown that highly confident
 or emerging rules are not individually robust. In figure 1, we compare the confidence
 (resp. growth rate) train values with the confidence (resp. growth rate) test values
 of rules extracted from UCI breast-w data set [Asuncion and Newman, 2007]."
TEng,article-1-4-teng,"Instability of interest measures. Although subsets of rules allow good predictions, it is easy to show that high-confidence or emerging rules are not individually robust. In Figure 1, we compare the confidence values (resp. Growth and lift rates (Brin et al., 1997)) in learning and testing rules extracted from the UCI breast-w database (Asuncion and Newman , 2007)."
French,article-10-1-fr,"Ces transducteurs, capturant les grandes régularités de traduction existant dans le domaine biomédical, sont ensuite utilisés pour traduire de nouveaux termes français en anglais et vice versa. Les évaluations menées montrent que le taux de bonnes traductions de notre technique se situe entre 52 et 67%. À travers un examen des erreurs les plus courantes, nous identifions quelques limites inhérentes à notre approche et proposons quelques pistes pour les dépasser. Nous envisageons enfin plusieurs extensions à ce travail."
English,article-10-1-eng,"Then, these transducers, making the most of high translation regularities in the biomedical domain, can be used to translate new French terms into English or vice versa. Evaluations reported show that our technique achieves good successful translation rates (between 52 and 67%). When examining at the most frequent errors made, some inherent limits of our approach are identified, and several avenues are proposed in order to bypass them. Finally, some perspectives are put forward to extend this work."
TEng,article-10-1-teng,"These transducers, capturing the main translation regularities existing in the biomedical field, are then used to translate new French terms into English and vice versa. Evaluations have shown that the rate of good translations of our technique is between 52 and 67%. Through an examination of the most common errors, we identify some limits inherent in our approach and suggest some ways to overcome them. We are finally considering several extensions to this work."
French,article-10-10-fr,"2. tous les préfixes communs des séquences de sortie sont ensuite remontés des feuilles vers la racine de l’arbre (figure 3).
3. enfin, en partant de la racine, tous les noeuds sont considérés deux à deux et fusionnés si le transducteur résultant n’entre pas en contradiction avec les données du jeu d’entraînement (figures 4 et 5). L’ordre de ces tentatives de fusion est généralement indiqué par une fonction heuristique. Il est possible de repousser des séquences de sortie vers les feuilles pour permettre des fusions. Quand plus aucune fusion n’est possible, l’algorithme termine."
English,article-10-10-eng,"2. all the common prefixes of the output sequences are then raised from the leaves to the root of the tree (Figure 3).
3. finally, starting from the root, all the nodes are considered in pairs and merged if the resulting transducer does not contradict the data of the training set (Figures 4 and 5). The order of these merging attempts is usually indicated by a heuristic function. It is possible to push output sequences towards the sheets to allow mergers. When no more merging is possible, the algorithm ends."
TEng,article-10-10-teng,"2. all the common prefixes of the output sequences are then raised from the leaves to the root of the tree (Figure 3).
3. finally, starting from the root, all the nodes are considered in pairs and merged if the resulting transducer does not contradict the data of the training set (Figures 4 and 5). The order of these merging attempts is usually indicated by a heuristic function. It is possible to push output sequences towards the sheets to allow mergers. When no more merging is possible, the algorithm ends."
French,article-10-13-fr,"Pour se focaliser sur les termes qui sont morphologiquement proches, la similarité formelle de chaque paire a été évaluée à l’aide d’une distance d’édition normalisée par la longueur des mots. Les paires sont classées dans une liste selon ce score en ordre décroissant de similarité :"
English,article-10-13-eng,"In order to focus on morphologically related pairs, a formal similarity was computed for each pair through the string edit distance. Pairs were then ordered in a list according to their scores in descending order."
TEng,article-10-13-teng,"To focus on terms that are morphologically close, the formal similarity of each pair was assessed using an editing distance normalized by the length of the words. The pairs are ranked in a list according to this score in descending order of similarity:"
French,article-10-15-fr,"Pour chacune des expériences, nous testons les sens de traductions français vers anglais et l’inverse, avec différentes tailles de jeux d’entraînement. Les jeux de test comportent 2000 paires (bien entendu différentes des paires d’entraînement) et les processus d’inférence et de validation sont répétés dix fois et les résultats moyennés. L’expérience 2 est une évaluation dans le pire des cas puisque les transducteurs inférés sont testés sur des paires qui peuvent n’être pas morphologiquement apparentées."
English,article-10-15-eng,"For each experiment, we test our approach for the translation of terms from French to English and from English to French. The inference process is repeated 10 times: the initial set is divided in 10 folds and ostia uses 9 of these folds; the tenth fold is different each time. Thus, ten transducers are inferred; this allows us to average their results (see Section 4). Each test set comprises 2000 pairs (of course different from the training pairs)."
TEng,article-10-15-teng,"For each of the experiments, we test the meanings of French to English translations and vice versa, with different sizes of training games. The test sets consist of 2000 pairs (of course different from the training pairs) and the inference and validation processes are repeated ten times and the results averaged. Experiment 2 is a worst-case assessment since the inferred transducers are tested on pairs which may not be morphologically related."
French,article-10-2-fr,"Introduction
Dans le domaine biomédical, l’évolution rapide des connaissances et la prédominance de l’anglais comme langue de communication rendent cruciales les problématiques de production et de gestion de ressources terminologiques multilingues. Dans ce cadre, la traduction de terminologies existantes, dont fait l’objet cet article, revêt une grande importance."
English,article-10-2-eng,"Introduction
In the biomedical domain, the international research framework and fast knowledge update make producing, managing and updating multilingual resources an important issue. Within this context, this paper presents and evaluates an original method to automatically translate a large class of biomedical simple terms"
TEng,article-10-2-teng,"Introduction
In the biomedical field, the rapid evolution of knowledge and the predominance of English as a language of communication make crucial the problems of production and management of multilingual terminological resources. In this context, the translation of existing terminologies, which is the subject of this article, is of great importance."
French,article-10-7-fr,"Un transducteur séquentiel est un transducteur dans lequel tous les états sont finals, et où il est impossible d’avoir deux arcs sortant d’un même état ayant le même symbole d’entrée. Cette dernière propriété est celle qui assure le déterminisme des traductions issues de ces transducteurs. Enfin, un transducteur sous-séquentiel est un transducteur séquentiel dans lequel à chaque état est associée une séquence de sortie. Celle-ci est produite lorsque la séquence d’entrée se termine sur l’état ; c’est ce qui rend les transducteurs sous-séquentiels relativement expressifs."
English,article-10-7-eng,"A sub-sequential transducer is a deterministic transducer (two edges with the same input symbol cannot emerge from the same state) in which all the states are final, and having an output string associated to each state. This string is produced when the input sequence to be accepted by the transducer ends on the state it is associated with."
TEng,article-10-7-teng,"A sequential transducer is a transducer in which all the states are final, and where it is impossible to have two arcs coming out of the same state having the same input symbol. This last property is that which ensures the determinism of the translations coming from these transducers. Finally, a sub-sequential transducer is a sequential transducer in which an output sequence is associated with each state. This is produced when the entry sequence ends on the state; this is what makes sub-sequential transducers relatively expressive."
French,article-11-1-fr,"Dans cet article, nous nous intéressons à l’identification automatique des paires de documents parallèles contenues dans un corpus bilingue. Nous montrons que cette tâche peut être accomplie avec précision en utilisant un ensemble restreint d’invariants lexicaux. Nous évaluons également notre approche sur une tâche de traduction automatique et montrons qu’elle obtient des résultats supérieures à un système de référence faisant usage d’un lexique bilingue."
English,article-11-1-eng,In this study we address the problem of automatically identifying the pairs of texts that are translation of each other in a set of documents. We show that it is possible to automatically build particularly efficient content-based methods that make use of very little lexical knowledge. We also evaluate our approach toward a front-end translation task and demonstrate that our parallel text classifier yields better performances than another approach based on a rich lexicon.
TEng,article-11-1-teng,"In this article, we are interested in the automatic identification of pairs of parallel documents contained in a bilingual corpus. We show that this task can be accomplished with precision using a small set of lexical invariants. We also evaluate our approach on a machine translation task and show that it achieves results superior to a reference system using a bilingual lexicon."
French,article-11-13-fr,"Résultats
Nous avons comparé les performances de notre moteur de TA lorsqu’il est entraîné sur quatre corpus parallèles différents. Contrairement à nos expériences sur EUROPARL, la mesure de cosinus et la distance d’édition ont des performances comparables. Cela pourrait s’expliquer par la plus petite taille des documents de PAHO (rendant l’ordre des caractéristiques moins important) et par l’étape de suppression des paires partageant un document."
English,article-11-13-eng,"Results
We compared the performance of our translation engine when trained on four different bitexts. Contrary to our former experiment, cosine and edit performed similarly well. This could be explained by the shorter length of the documents and by the filtering step applied on the parallel pairs, which seems to eliminate untrusted pairs. Inspection of their bitexts showed that they shared only 229 document pairs, so we trained another translation engine on the union of those bitexts. The scores of all the translation engines are reported in Table 2."
TEng,article-11-13-teng,"Results
We compared the performance of our AT engine when it is trained on four different parallel corpora. Contrary to our experiences on EUROPARL, the cosine measurement and the editing distance have comparable performances. This could be explained by the smaller size of PAHO documents (making the order of characteristics less important) and by the step of removing pairs sharing a document."
French,article-11-2-fr,"Introduction
De nos jours, les corpus de documents parallèles (ensemble de documents exprimant le même contenu dans le même ordre) jouent un rôle crucial dans les applications multilingues de traitement automatique des langues. Aligné au niveau des phrases, une tâche pouvant être accomplie avec fiabilité, un corpus parallèle s’avère très utile aux concordanciers bilingues et est la pierre angulaire de la plupart des systèmes commerciaux de mémoire de traduction."
English,article-11-2-eng,"Introduction
Parallel corpora are currently playing a crucial role in multilingual natural language processing applications. Aligned at the sentence level, a task that has been shown to be fairly easy, a parallel corpus turns out to be already very useful for bilingual concordancers and is the cornerstone of most of the commercial translation memory systems that have been and still are popular among professional translators."
TEng,article-11-2-teng,"Introduction
Nowadays, the corpus of parallel documents (set of documents expressing the same content in the same order) play a crucial role in multilingual applications of automatic language processing. Aligned at the sentence level, a task that can be reliably accomplished, a parallel corpus is very useful for bilingual concorders and is the cornerstone of most commercial translation memory systems."
French,article-11-5-fr,Nous évaluons également notre approche à travers une tâche de traduction automatique et mesurons des performances supérieures à celles d’une approche faisant usage d’un lexique bilingue riche (section 4). Nous discutons en section 5 de travaux connexes et présentons en section 6 nos conclusions.
English,article-11-5-eng,"In sections 3 and 4 we evaluate our approaches on two tasks: a controlled one on a part of the europarl corpus, as well as a real task we faced when developing an English- Spanish concordancer.We then show that some of the approaches we investigated are very effective at identifying parallel texts and that their use for seeding a translation engine is also fruitful."
TEng,article-11-5-teng,We also evaluate our approach through an automatic translation task and measure performance superior to that of an approach using a rich bilingual lexicon (section 4). We discuss in section 5 related work and present in section 6 our conclusions.
French,article-11-9-fr,"Expérience contrôlée EUROPARL est un corpus parallèle tiré de la transcription des débats parlementaires européens s’étant tenus entre avril 1996 et septembre 2003 (Koehn, 2002). Les débats parlementaires européens sont traduits en onze langues, mais nous nous sommes concentrés sur les traductions anglaises et espagnoles. Notre corpus était composé de 487 textes anglais et de 487 textes espagnols ayant en moyenne environ 2800 phrases chacun."
English,article-11-9-eng,"Controlled Task
europarl is a large corpus of bitexts drawn from the European Parliament between April 1996 and September 2003 [15]. It includes versions of the documents in 11 languages, but we focus in this study on the English-Spanish bitext. Our test corpus contains 487 English documents (therefore 487 Spanish ones), thus summing to 237,169 potential pairs of documents. Each document contains an average of about 2,800 sentences."
TEng,article-11-9-teng,"Controlled experience EUROPARL is a parallel corpus drawn from the transcription of European parliamentary debates held between April 1996 and September 2003 (Koehn, 2002). European parliamentary debates are translated into eleven languages, but we have focused on English and Spanish translations. Our corpus was made up of 487 English texts and 487 Spanish texts with an average of approximately 2800 sentences each."
French,article-13-0-fr,Résumé PrepLex est un lexique des prépositions du français. Il contient les informations utiles à des systèmes d’analyse syntaxique. Il a été construit en comparant puis fusionnant différentes sources d’informations lexicales disponibles. Ce lexique met également en évidence les prépositions ou classes de prépositions qui apparaissent dans la définition des cadres de sous-catégorisation des ressources lexicales qui décrivent la valence des verbes.
English,article-13-0-eng,Abstract PrepLex is a lexicon of French prepositions which provides all the information needed for parsing. It was built by comparing and merging several authoritative lexical sources. This lexicon also shows the prepositions or classes of prepositions that appear in verbs subcategorization frames.
TEng,article-13-0-teng,Summary PrepLex is a lexicon of French prepositions. It contains information useful for parsing systems. It was built by comparing and then merging different sources of lexical information available. This lexicon also highlights the prepositions or classes of prepositions that appear in the definition of lexical resource sub-categorization frameworks that describe the valence of verbs.
French,article-13-10-fr,"Notre travail est destiné à fournir un lexique utilisable par un analyseur syntaxique. Nous nous sommes restreints aux aspects purement syntaxiques et à quelques éléments sémantiques comme la définition des ensembles de prépositions ayant un aspect sémantique commun (comme LOC). Le lexique produit est diffusé sous licence libre et a vocation à être intégré dans des ressources plus larges, existantes ou à venir."
English,article-13-10-eng,"Our work aims at providing the community with a lexicon that can be directly used by a parser. We focused on syntactic aspects and extended the work to some semantic elements, like semantically linked sets of prepositions (as LOC). The generated lexicon is freely available and is expected to be integrated into larger resources for French, whether existing or under development."
TEng,article-13-10-teng,"Our work is intended to provide a lexicon usable by a syntactic analyzer. We have limited ourselves to purely syntactic aspects and to a few semantic elements such as the definition of sets of prepositions with a common semantic aspect (like LOC). The lexicon produced is distributed under a free license and is intended to be integrated into larger resources, existing or future."
French,article-13-15-fr,"Nous avons donc effectué, dans un premier temps, un travail d’inventaire des prépositions présentes dans un certain nombre de ressources, des lexiques et/ou dictionnaires d’une part, pour la liste générale, des lexiques syntaxiques d’autre part, pour la liste de prépositions argumentales. Deux ressources se classent cependant dans les deux catégories, le Lefff et le dictionnaire UNL :"
English,article-13-15-eng,"We thus collected, as a first step, prepositions from a certain number of resources, lexicons and dictionaries for the garden-variety list, and syntactic lexicons for the argument prepositions list. Two resources belong to both categories, Lefff and French- UNL dictionary:
"
TEng,article-13-15-teng,"We therefore carried out, as a first step, an inventory of the prepositions present in a certain number of resources, lexicons and / or dictionaries on the one hand, for the general list, syntactic lexicons on the other hand, for the list of argument prepositions. However, two resources fall into two categories, the Lefff and the UNL dictionary:"
French,article-13-17-fr,"UNL (Universal Networking Language (Sérasset & Boitet, 2000)), est un dictionnaire du français vers un anglais désambiguïsé conçu pour la traduction automatique, qui comprend des informations syntaxiques dans sa partie française. UNL n’a qu’une couverture assez faible (moins de 27 000 lemmes), mais il propose dans sa partie anglaise des informations de type sémantique que nous envisageons d’utiliser par la suite. UNL contient 48 prépositions simples dont 10 apparaissant dans les cadres de sous-catégorisation des verbes."
English,article-13-17-eng,"UNL (Universal Networking Language (S´erasset and Boitet, 2000)), is a French to disambiguated English dictionary for machine translation, which contains syntactic information in its French part (see table 1 for a UNL example entry). UNL has limited coverage (less than 27,000 lemmas), but it provides, in the English part, semantic information that we will consider using in the near future. UNL contains 48 simple prepositions, among which 12 appear in verb subcategorization frames."
TEng,article-13-17-teng,"UNL (Universal Networking Language (Sérasset & Boitet, 2000)), is a disambiguated French to English dictionary designed for machine translation, which includes syntactic information in its French part. UNL has only fairly weak coverage (less than 27,000 lemmas), but it offers semantic information in its English part which we plan to use later. UNL contains 48 simple prepositions, 10 of which appear in the sub-categorization of verbs."
French,article-17-0-fr,"Dans le cadre du projet Papillon qui vise à la construction de bases lexicales multilingues par acceptions, nous avons défini des stratégies pour peupler un dictionnaire pivot de liens interlingues à partir d’une base vectorielle monolingue. Il peut y avoir un nombre important de sens par entrée et donc l’identification des acceptions correspondantes peut être erronée. Nous améliorons l’intégrité de la base d’acception grâce à des agents experts dans les fonctions lexicales comme la synonymie, l’antonymie, l’hypéronymie ou l’holonymie. Ces agents sont capable de calculer la pertinence d’une relation sémantique entre deux acceptions par les diverses informations lexicales récoltées et les vecteurs conceptuels."
English,article-17-0-eng,"In the framework of the Papillon project, we have defined strategies for populating a pivot dictionnary of interlingual links from monolingual vectorial bases. There are quite a number of acceptions per entry thus, the proper identification may be quite troublesome and some added clues beside acception links may be useful. We improve the integrity of the acception base through well known semantic relations like synonymy, antonymy, hyperonymy and holonymy relying on lexical functions agents. These semantic relation agents can compute the pertinence of a semantic relation between two acceptions thanks to various lexical informations and conceptual vectors."
TEng,article-17-0-teng,"Within the framework of the Papillon project which aims at building multilingual lexical bases by acceptances, we defined strategies to populate a pivot dictionary of interlingual links from a monolingual vector base. There can be a significant number of meanings per entry and therefore the identification of the corresponding meanings may be incorrect. We improve the integrity of the acceptance basis thanks to agents expert in lexical functions such as synonymy, anonymity, hyperonymy or holonymy. These agents are capable of calculating the relevance of a semantic relationship between two meanings by the various lexical information collected and the conceptual vectors."
French,article-17-14-fr,"Liens sémantiques
Les relations sémantiques entre items lexicaux structurent le lexique sur le plan paradigmatique. Ces relations sont de deux types : les relations hiérarchiques (hyponymie/hypéronymie, méronymie/holonymie) et les relations d’équivalence/opposition (synonymie, antonymie). Elles sont souvent décrites comme des relations booléennes, i.e, elles existent entre deux items ou non (Polguère, 2001). Dans le cas des acceptions, monosémiques par définitions, les relations hiérarchiques sont transitives et la synonymie peut être considérée comme une équivalence."
English,article-17-14-eng,"Semantic Links and Integrity Constraints
The semantic relations between lexical units help structuring the lexicon on the paradigmatic plane. These well known relations are often described in two types: the hierarchical relations (hyponymy, hyperonymy, meronymy and holonymy) and the equivalence/opposition relations (synonymy, antonymy). In some linguistic research ([Polguere, 2001 ], [Lehmann et Martin-Berthet, 98]) they are defined as boolean relations i.e. they either hold between two words or they don't. For acceptions, which are monosemic, hierarchical relations are transitive and synonymy is considered as an equivalence."
TEng,article-17-14-teng,"Semantic links
The semantic relationships between lexical items structure the lexicon on a paradigmatic level. These relationships are of two types: hierarchical relationships (hyponymy / hyperonymy, meronymy / holonymy) and equivalence / opposition relationships (synonymy, antonymy). They are often described as Boolean relationships, i.e., they exist between two items or not (Polguère, 2001). In the case of acceptances, monosemic by definition, hierarchical relationships are transitive and synonymy can be considered as an equivalence."
French,article-17-18-fr,"Création et suppression de liens
Nous voulons ajouter des liens sémantiques à la base d’acceptions afin, non seulement de constituer un réseau sémantique mais également pour pouvoir vérifier son intégrité. Les agents capables d’évaluer une relation sémantique valuée peuvent créer des liens sémantiques valués si la valuation est supérieure à un seuil s. Par exemple, si un agent expert d’antonymie évalue l’antonymie entre ,froidet ,chaud- à une valeur n supérieure à s, il construit un lien sémantique entre les deux acceptions valué à n."
English,article-17-18-eng,"Links Creation and Deletion
We want to add semantics links to the acception base in order to assess the integrity of the base. Agents which can evaluate a given semantic relation can create valued semantics links if the valuation result is above a threshold th. For exemple, if one antonymy agent evaluates antonymy between ,cold- and ,hot- at a value above th, it builds a semantic link between the two acceptions valued at th. This threshold is not fixed in advance, and it evolves according to the number of links already built."
TEng,article-17-18-teng,"Creation and removal of links
We want to add semantic links to the acceptance base in order not only to constitute a semantic network but also to be able to verify its integrity. Agents capable of evaluating a valued semantic relationship can create valued semantic links if the valuation is greater than a threshold s. For example, if an expert anonymity agent evaluates the anonymity between, cold and warm - at a value n greater than s, he builds a semantic link between the two meanings valued at n."
French,article-17-19-fr,Le seuil n’est pas fixé en avance et il évolue constament en fonction du nombre de liens déjà construits. Le système apprend en fonction des nouvelles données (nouveaux dictionnaires monolingues ou bilingues) ou en révisant les anciennes données et donc les agents doivent régulièrement recalculer les relations et si la condition pour préserver le lien (être supérieur à s) n’est plus respectée alors le lien est détruit.
English,article-17-19-eng,"Thus, this value will evolve during time. The system learns from new data (new monolingual or bilinguals dictionnaries) or by revising old data so agents can compute again a relation and if the condition to preserve the link, to be above th, is not verified the link is deleted."
TEng,article-17-19-teng,The threshold is not set in advance and it is constantly changing depending on the number of links already built. The system learns according to the new data (new monolingual or bilingual dictionaries) or by revising the old data and therefore the agents must regularly recalculate the relationships and if the condition to preserve the link (be greater than s) is no longer met the link is destroyed.
French,article-17-2-fr,"Introduction
La recherche en représentation de sens est un important problème qui a été abordé selon plusieurs approches. Notre équipe travaille actuellement sur l’analyse thématique de textes et la désambiguïsation lexicale (Lafourcade, 2001). Nous construisons un système capable d’apprentissage automatique basé sur les vecteurs conceptuels. Les vecteurs contiennent les idées associées aux mots ou expressions. Le système d’apprentissage construit ou révise automatiquement les vecteurs conceptuels à partir de définitions en langage naturel contenues dans les dictionnaires à usage humain."
English,article-17-2-eng,"Introduction
Research in meaning representation in NLP is an important problem still addressed through several approaches. The NLP team from LIRMM currently works on thematic text analysis and lexical disambiguation [Lafourcade, 2001]. To this purpose, we built a system, with automated learning capabilities, based on conceptual vectors for meaning representation. Vectors are supposed to encode `ideas' associated to words or expressions. The conceptual vectors learning system automatically defines or revises its vectors from definitions in natural language contained in electronic dictionaries for human usage."
TEng,article-17-2-teng,"Introduction
Research into the representation of meaning is an important problem which has been approached according to several approaches. Our team is currently working on thematic text analysis and lexical disambiguation (Lafourcade, 2001). We are building a system capable of machine learning based on conceptual vectors. The vectors contain the ideas associated with the words or expressions. The learning system automatically constructs or revises conceptual vectors from natural language definitions contained in dictionaries for human use."
OG_ENG_ESSAY,og-eng-1,"There have been many studies in which researchers have attempted to classify student attentiveness. Many of these approaches depended on a qualitative analysis and lacked any quantitative analysis. Therefore, this work is focused on bridging the gap between qualitative and quantitative approaches to classify student attentiveness. Thus, this research applies machine learning algorithms (K-means and SVM) to automatically classify students as attentive or inattentive using data from a consumer RGB-D sensor. Results of this research can be used to improve teaching strategies for instructors at all levels and can aid instructors in implementing personalized learning systems, which is a National Academy of Engineering Grand Challenge. This research applies machine learning algorithms to an educational setting. Data from these algorithms can be used by instructors to provide valuable feedback on the effectiveness of their instructional strategies and pedagogies. Instructors can use this feedback to improve their instructional strategies, and students will benefit by achieving improved learning and subject mastery. Ultimately, this will result in the students' increased ability to do work in their respective areas. Broadly, this work can help advance efforts in many areas of education and instruction. It is expected that improving instructional strategies and implementing personalized learning will help create more competent, capable, and prepared persons available for the future workforce."
OG_ENG_ESSAY,og-eng-2,"Decision tree induction is one of the useful approaches for extracting classification knowledge from a set of feature-based instances. The most popular heuristic information used in the decision tree generation is the minimum entropy. This heuristic information has a serious disadvantage-the poor generalization capability [3]. Support vector machine (SVM) is a classification technique of machine learning based on statistical learning theory. It has good generalization. Considering the relationship between the classification margin of support vector machine(SVM) and the generalization capability, the large margin of SVM can be used as the heuristic information of decision tree, in order to improve its generalization capability. This paper proposes a decision tree induction algorithm based on large margin heuristic. Comparing with the binary decision tree using the minimum entropy as the heuristic information, the experiments show that the generalization capability has been improved by using the new heuristic."
OG_ENG_ESSAY,og-eng-3,"This paper firstly analyses the actual underwriting methods of Chinese life insurance companies, and points out the merits and shortcomings of these methods. Then the incomplete database of insurance company is mined by the data mining's association rule algorithm. Thirdly the support vector machine (SVM) is applied to the underwriting process to classify the applicants. Finally the directions for improving this algorithm are pointed out. The algorithm proposed in this paper has promising future in underwriting process."
OG_ENG_ESSAY,og-eng-4,"This paper presents a novel machine learning model-kernel granular support vector machine (KGSVM), which combines traditional support vector machine (SVM) with granular computing theory. By dividing granules and replacing with them in kernel space, the datasets can be reduced effectively without changing data distribution. And then the generalization performance and training efficiency of SVM can be improved. Simulation results on UCI datasets demonstrate that KGSVM is highly scalable for large datasets and very effective in terms of classification."
OG_ENG_ESSAY,og-eng-5,"Determination of model complexity is a challenging issue to solve computer vision problems using restricted boltzmann machines (RBMs). Many algorithms for feature learning depend on cross-validation or empirical methods to optimize the number of features. In this work, we propose an learning algorithm to find the optimal model complexity for the RBMs by incrementing the hidden layer. The proposed algorithm is composed of two processes: 1) determining incrementation necessity of neurons and 2) computing the number of additional features for the increment. Specifically, the proposed algorithm uses a normalized reconstruction error in order to determine incrementation necessity and prevent unnecessary increment for the number of features during training. Our experimental results demonstrated that the proposed algorithm converges to the optimal number of features in a single layer RBMs. In the classification results, our model could outperform the non-incremental RBM."
OG_ENG_ESSAY,og-eng-6,"Stock market or Share market is one of the most complicated and sophisticated way to do business. Small ownerships, brokerage corporations, banking sector, all depend on this very body to make revenue and divide risks; a very complicated model. However, this paper proposes to use machine learning algorithm to predict the future stock price for exchange by using open source libraries and preexisting algorithms to help make this unpredictable format of business a little more predictable. We shall see how this simple implementation will bring acceptable results. The outcome is completely based on numbers and assumes a lot of axioms that may or may not follow in the real world so as the time of prediction."
OG_ENG_ESSAY,og-eng-7,"We have investigated the risk factors that lead to severe retinopathy of prematurity using statistical analysis and logistic regression as a form of generalized additive model (GAM) with pairwise interaction terms (GA2M). In this process, we discuss the trade-off between accuracy and interpretability of these machine learning techniques on clinical data. We also confirm the intuition of expert neonatologists on a few risk factors, such as gender, that were previously deemed as clinically not significant in RoP prediction."
OG_ENG_ESSAY,og-eng-8,"This work proposes an intelligent learning diagnosis system that supports a Web-based thematic learning model, which aims to cultivate learners' ability of knowledge integration by giving the learners the opportunities to select the learning topics that they are interested, and gain knowledge on the specific topics by surfing on the Internet to search related learning courseware and discussing what they have learned with their colleagues. Based on the log files that record the learners' past online learning behavior, an intelligent diagnosis system is used to give appropriate learning guidance to assist the learners in improving their study behaviors and grade online class participation for the instructor. The achievement of the learners' final reports can also be predicted by the diagnosis system accurately. Our experimental results reveal that the proposed learning diagnosis system can efficiently help learners to expand their knowledge while surfing in cyberspace Web-based ""theme-based learning"" model."
OG_ENG_ESSAY,og-eng-9,"Semi-supervised support vector machine is an extension of standard support vector machine in machine learning problem in real life. However, the existing semi-supervised support vector machine algorithm has some drawbacks such as slower training speed, lower accuracy, etc. This paper presents a semi-supervised support vector machine learning algorithm based on active learning, which trains early learner by a spot of labeled-data, selects the best training samples for training and learning by active learning and reduces learning cost by deleting non- support vector. Simulative experiments have shown that the algorithm may get good learning effect at less learning cost."
OG_ENG_ESSAY,og-eng-10,"Transfer learning aims to improve a targeted learning task using other related auxiliary learning tasks and data. Most current transfer-learning methods focus on scenarios where the auxiliary and the target learning tasks are very similar: either (some of) the auxiliary data can be directly used as training examples for the target task or the auxiliary and the target data share the same representation. However, in many cases the connection between the auxiliary and the target tasks can be remote. Only a few features derived from the auxiliary data may be helpful for the target learning. We call such scenario the deep transfer-learning scenario and we introduce a novel transfer-learning method for deep transfer. Our method uses restricted Boltzmann machine to discover a set of hierarchical features from the auxiliary data. We then select from these features a subset that are helpful for the target learning, using a selection criterion based on the concept of kernel-target alignment. Finally, the target data are augmented with the selected features before training. Our experiment results show that this transfer method is effective. It can improve classification accuracy by up to more than 10%, even when the connection between the auxiliary and the target tasks is not apparent."
OG_ENG_ESSAY,og-eng-11,"IASS is the integrated anti-spam system, which adopts machine learning to filter spam in a intelligent, flexible, precise, and self-adaptive way. The methods of linear classification based on optimal separating hyperplane and K-means clustering are used in action recognition layer. The method of improved naive Bayes is used in content analysis layer. The application of machine learning helps improve the performance of IASS"
OG_ENG_ESSAY,og-eng-12,"Single machine scheduling methods have attracted a lot of attentions in recent years. Most dynamic single machine scheduling problems in practice have been addressed using dispatching rules. However, no single dispatching rule has been found to perform well for all important criteria, and no rule takes into account the status or the other resources of system's environment. In this research, an intelligent agent-based single machine scheduling system is proposed, where the agent is trained by a new improved Q-learning algorithm. In such scheduling system, agent selects one of appropriate dispatching rules for machine based on available information. The agent was trained by a new simulated annealing-based Q-learning algorithm. The simulation results show that the simulated annealing-based Q-learning agent is able to learn to select the best dispatching rule for different system objectives. The results also indicate that simulated annealing-based Q-learning agent could perform well for all criteria, which is impossible when using only one dispatching rule independently."
OG_ENG_ESSAY,og-eng-13,"The objective of this work is to present a machine learning (ML) -based framework to identify evidence about collaborative problem solving (CPS) cognitive (teamwork) and social-emotional learning (SEL) skills from the dyadic (human-human-HH) interactions. This work extends our previous work (Chopade et al. IEEE HST 2018, LAK2019) [1], [2]. Explicitly, we are interested in how teamwork skills and team dynamics are demonstrated as verbal and nonverbal behaviors, and how these behaviors can be captured and analyzed via passive data collection. For this work we use a two-player cooperative CPS game, Crisis in Space (CIS) from LRNG (Previously GlassLab Inc). During the summer of 2018, we implemented this CIS game for interns as a group study. A total of 34 participants played the game and provided study and survey data. During the study, we collected participants' game play data, such as audio, video and eye tracking data streams. This research involves analyzing CIS multimodal game data, and developing skill models, and machine learning techniques for CPS skills measurement. In this paper, we present our ML framework for the analysis of audio data along with preliminary results from a pilot study. The analysis of audio data uses natural language processing (NLP) techniques, such as bag-of-words and sentence embedding. Our preliminary results show that various NLP features can be used to describe successful and unsuccessful CPS performances. The ML based framework supports the development of evidence centered design for teamwork skills-mapping and aims to help teams operate effectively in a complex situation. Potential applications of this work include support for the Department of Homeland Security (DHS), and the US Army for the development of learner and team centric training, cohort, and team behavioral skill-mapping."
OG_ENG_ESSAY,og-eng-14,"Representation learning is the base and crucial for consequential tasks, such as classification, regression, and recognition. The goal of representation learning is to automatically learning good features with deep models. Multimodal representation learning is a special representation learning, which automatically learns good features from multiple modalities, and these modalities are not independent, there are correlations and associations among modalities. Furthermore, multimodal data are usually heterogeneous. Due to the characteristics, multimodal representation learning poses many difficulties: how to combine multimodal data from heterogeneous sources; how to jointly learning features from multimodal data; how to effectively describe the correlations and associations, etc. These difficulties triggered great interest of researchers along with the upsurge of deep learning, many deep multimodal learning methods have been proposed by different researchers. In this paper, we present an overview of deep multimodal learning, especially the approaches proposed within the last decades. We provide potential readers with advances, trends and challenges, which can be very helpful to researchers in the field of machine, especially for the ones engaging in the study of multimodal deep machine learning."
OG_ENG_ESSAY,og-eng-15,"This study investigates the use of tree-based machine learning methods in concrete non-destructive tests (NDT) evaluation. The study encompassed different phases. The first involved the use of destructive and non-destructive mechanisms to assess concrete strength on cube specimens. The second phase examined site assessment of selected structures using popular NDT tools. The third phase implemented a tree-based machine learning approach to characterize a relationship between concrete properties and destructive compressive strength for cubes and selected structures. It established that ultrasonic speed and rebound number were adequate to predict compressive strength. Variable importance plots from boosted tree learning suggested a hierarchy of parameter importance that challenges Pearson's correlation coefficients. In order to establish the effectiveness of tree-methods, analyses show that classical regression struggled to attain a variance score of 0.43 during training while boosted tree doubled this score during testing on unseen validation set. The results present a case for tree-based analysis in concrete NDT evaluation."
OG_ENG_ESSAY,og-eng-16,"Today, social networks have been part of many people's lives. Many activities such as communication, promotion, advertisement, news, agenda creation have started to be done through social networks. Some malicious accounts on Twitter are used for purposes such as misinformation and agenda creation. This is one of the basic problems in social networks. Therefore, detection of malicious account is significant. In this study, machine learning-based methods were used to detect fake accounts that could mislead people. For this purpose, the dataset generated was pre-processed and fake accounts were determined by machine learning algorithms. Decision trees, logistic regression and support vector machines algorithms are used for the detection of fake accounts. Classification performances of these methods are compared and the logistic regression proved to be more successful than the others."
OG_ENG_ESSAY,og-eng-17,"The theory of machine learning in metric space is a new research topic and has drawn much attention in recent years. The theoretical foundation of this topic is the question under which conditions two sample sets can be separated in this space. In this paper, motivated by developing a new support vector machine (SVM) in fuzzy number space, we present a necessary and sufficient condition of separating two finite classes of samples by a hyper-plane in n-dimensional fuzzy number space. We also present an attainable expression of maximal margin of the separating hyper-planes which includes some cases of the classes of infinite samples in n-dimensional fuzzy number space. These results generalize and improve the corresponding conclusions for the theory of SVM in Hilbert space to fuzzy number space."
OG_ENG_ESSAY,og-eng-18,"The traditional SVM does not support incremental learning. And the traditional training method of SVM is not working when the amount of training samples are so large that they can not be put into the RAM of computer. In order to solve this problem and improve the speed of training SVM, the natural characteristics of SV are analyzed in this paper. An incremental learning algorithm (I-SVM) for SVM with discarding part of history samples is presented. The theoretical analysis and experimental results show that this algorithm can not only speed up the training process, but also reduce the storage cost, while the classification precision is also guaranteed."
OG_ENG_ESSAY,og-eng-19,"This paper presents an exploratory machine learning attack based on deep learning to infer the functionality of an arbitrary classifier by polling it as a black box, and using returned labels to build a functionally equivalent machine. Typically, it is costly and time consuming to build a classifier, because this requires collecting training data (e.g., through crowdsourcing), selecting a suitable machine learning algorithm (through extensive tests and using domain-specific knowledge), and optimizing the underlying hyperparameters (applying a good understanding of the classifier's structure). In addition, all this information is typically proprietary and should be protected. With the proposed black-box attack approach, an adversary can use deep learning to reliably infer the necessary information by using labels previously obtained from the classifier under attack, and build a functionally equivalent machine learning classifier without knowing the type, structure or underlying parameters of the original classifier. Results for a text classification application demonstrate that deep learning can infer Naive Bayes and SVM classifiers with high accuracy and steal their functionalities. This new attack paradigm with deep learning introduces additional security challenges for online machine learning algorithms and raises the need for novel mitigation strategies to counteract the high fidelity inference capability of deep learning."
OG_ENG_ESSAY,og-eng-20,"We present two input data preprocessing methods for machine learning (ML). The first one consists in extending the set of attributes describing objects in input data table by new attributes and the second one consists in replacing the attributes by new attributes. The methods utilize formal concept analysis (FCA) and boolean factor analysis, recently described by FCA, in that the new attributes are defined by so-called factor concepts computed from input data table. The methods are demonstrated on decision tree induction. The experimental evaluation and comparison of performance of decision trees induced from original and preprocessed input data is performed with standard decision tree induction algorithms ID3 and C4.5 on several benchmark datasets."
OG_ENG_ESSAY,og-eng-21,In this paper we discuss various machine learning approaches used in mining of data. Further we distinguish between symbolic and sub-symbolic data mining methods. We also attempt to propose a hybrid method with the combination of Artificial Neural Network (ANN) and Cased Based Reasoning (CBR) in mining of data.
OG_ENG_ESSAY,og-eng-22,"Due to the complexity and flexibility of natural language, automatic linguistic knowledge acquisition and its application research becomes difficult. In this paper, we present a machine learning method to automatically acquire Chinese linguistic ontology knowledge from typical corpus. This study, first, defined the description frame of Chinese linguistic ontology knowledge, and then, automatically acquired the usage of a Chinese word with its co-occurrence of context in using semantic, pragmatics, syntactic, etc from the corpus, final, the above information and their representation act as Chinese linguistic ontology knowledge bank. We completed two groups of experiments, i.e. documents similarity computing, text reordering for information retrieval. Compared with previous works, the proposed method solves the inferior precision of nature language processing."
OG_ENG_ESSAY,og-eng-23,"Summary form only given. Learning is becoming the central problem in trying to understand intelligence and in trying to develop intelligent machines. The paper outlines some previous efforts in developing machines that learn. It sketches the authors's work on statistical learning theory and theoretical results on the problem of classification and function approximation that connect regularization theory and support vector machines. The main application focus is classification (and regression) in various domains-such as sound, text, video and bioinformatics. In particular, the paper describe the evolution of a trainable object detection system for classifying objects-such as faces and people and cars-in complex cluttered images. Finally, it speculates on the implications of this research for how the brain works and review some data which provide a glimpse of how 3D objects are represented in the visual cortex."
OG_ENG_ESSAY,og-eng-24,"In this paper, we use a deep learning method, restricted Boltzmann machine, for nonlinear system identification. The neural model has deep architecture and is generated by a random search method. The initial weights of this deep neural model are obtained from the restricted Boltzmann machines. To identify nonlinear systems, we propose special unsupervised learning methods with input data. The normal supervised learning is used to train the weights with the output data. The modified algorithm is validated by modeling two benchmark systems."
OG_ENG_ESSAY,og-eng-25,"Based on statistical learning theory (SLT), support vector machine (SVM), which is a new kind of machine learning method that is used for classification and regression. SVM is considered as two layers learning machine since it maps the original space into a high dimensional feature space, i.e., input layer and high dimensional feature space layer. If the high dimensional feature space layer is considered as a new problem's input layer and the new problem is also solved by SVM, the new problem can be solved by SVMs named multi-layer SVM (MLSVM). MLSVM is composed of input layer and at least one layer high dimensional feature space layer. In this paper, m-th order ordinary differential equations are solved by MLSVM for regression. Experimental results indicate that MLSVM can effectively solve the problem of ordinary differential equations. Thus, MLSVM exhibits its great potential to solve other complex problems"
OG_ENG_ESSAY,og-eng-26,"The support vector machines are the new statistical learning algorithm which is developed in recent years. They have some advantages in many regions like pattern recognition. The kernel function is important to its classification ability. This paper presents a crossbreed genetic algorithm based method to choose the kernel function and its parameters. The crossbreed genetic algorithm uses two fitness functions which are produced according to the two criterion of SVM's performance. The experiments proved that this algorithm can find effectively the optimal kernel function and its parameters, and it is helpful to increase the support vector machines' performance in fact."
OG_ENG_ESSAY,og-eng-27,"The importance of vocational and technical training is growing day by day in parallel to the developing technology. It is inevitable to utilise opportunities presented by information and communication technologies in order to determine vocational fields in vocational and technical training in the most efficient manner. In this respect, it is possible to create a more efficient tool compared to the current methods by utilising machine learning which is an artificial intelligence model in energy applications that predicts events in the future depending on the past experiences. In the current study, a software is developed that ensures that the system learns about the successful and unsuccessful choices made in the past by applying “Naive Bayes” algorithm, which is a machine learning algorithm, to the data collected concerning the individuals who turned out to be successful or unsuccessful in the vocational technical training process in energy applications. In the software developed, it is aimed that the system recommends the most suitable vocational field for the individual by according to the data collected from the individual who is in the occupation selection process in field energy applications."
OG_ENG_ESSAY,og-eng-28,"Support vector machine (SVM) is novel type learning machine, based on statistical learning theory, which tasks involving classification, regression or novelty detection. This paper investigates an inverse problem of support vector machines (SVMs). The inverse problem is how to split a given dataset into two clusters such that the margin between the two clusters attains the maximum. Here the margin is defined according to the separating hyper-plane generated by support vectors. It is difficult to give an exact solution to this problem. In this paper, we design a genetic algorithm to solve this problem. Numerical simulations show the feasibility and effectiveness of this algorithm. This study on the inverse problem of SVMs is motivated by designing a heuristic algorithm for generating decision trees with high generalization capability."
OG_ENG_ESSAY,og-eng-29,"Support vector machine (SVM) has become a popular tool of pattern recognition in recent years for its outstanding learning performance. When dealing with large-scale learning problems, incremental SVM framework is generally used because SVM can summarize the data space in a concise way. This paper proposes a training algorithm of incremental SVM with recombining method. Considering the differences of data distribution and the impact of new training data on history data, the history training dataset and the new training one are divided into independent groups and are recombined to train a classifier. In fact, this method can be implemented in a parallel structure for the actions of dividing may decrease the computation complexity of training a SVM. Meanwhile, the actions of recombining may weaken the potential impact caused by the difference of data distribution. The experiment results on a text dataset show that this training algorithm is effective and the classification accuracy of proposed incremental algorithm is superior to that using batch SVM model."
OG_ENG_ESSAY,og-eng-30,"A new method of early fault diagnosis for manufacturing system based on machine learning is presented. It is necessary for manufacturing enterprises to detect the states of production process in real time, in order to find the early faults in machines, so that the losses of production failure and investments of facility maintenance can be minimized. This paper proposes a new fault diagnosis model, which extracts multi-dimension features from the detected signal to supervise the different features of the signal simultaneously. Based on the model, the method of inductive learning is adopted to obtain the statistical boundary vectors of the signal automatically, and then a normal feature space is built, according to which an abnormal signal can be detected, and consequently the faults in a complicated system can be found easily. Furthermore, under the condition of without existing fault samples, the precise results of fault diagnosis can also be achieved in real time. The theoretical analysis and simulation example demonstrate the effectiveness of the method."
OG_ENG_ESSAY,og-eng-31,"Cyber bullying is the use of technology as a medium to bully someone. Although it has been an issue for many years, the recognition of its impact on young people has recently increased. Social networking sites provide a fertile medium for bullies, and teens and young adults who use these sites are vulnerable to attacks. Through machine learning, we can detect language patterns used by bullies and their victims, and develop rules to automatically detect cyber bullying content. The data we used for our project was collected from the website Formspring.me, a question-and-answer formatted website that contains a high percentage of bullying content. The data was labeled using a web service, Amazon's Mechanical Turk. We used the labeled data, in conjunction with machine learning techniques provided by the Weka tool kit, to train a computer to recognize bullying content. Both a C4.5 decision tree learner and an instance-based learner were able to identify the true positives with 78.5% accuracy."
OG_ENG_ESSAY,og-eng-32,"A novel real-time acoustic feedback (RTAF) based on machine learning to reduce the duration and to improve the progress in the rehabilitation is presented. Wearable technology (WT) has emerged as a viable means to provide low-cost digital healthcare and therapy course outside the medical environment like hospitals and clinics. In this paper we show that the RTAF together with WTs can offer an excellent solution to be used in rehabilitation. The method of RTAF based on machine learning as well as a study for proving its effectiveness are presented below. The results show a faster recovery time using RTAF. The proposed RTAF shows a great potential to be used and deployed to support digital healthcare, therapy and rehabilitation."
OG_ENG_ESSAY,og-eng-33,"In this paper, we present a framework which enables medical decision making in the presence of partial information. At its core is ontology-based automated reasoning, machine learning techniques are integrated to enhance existing patient datasets in order to address the issue of missing data. Our approach supports interoperability between different health information systems. This is clarified in a sample implementation that combines three separate datasets (patient data, drug-drug interactions and drug prescription rules) to demonstrate the effectiveness of our algorithms in producing effective medical decisions. In short, we demonstrate the potential for machine learning to support a task where there is a critical need from medical professionals by coping with missing or noisy patient data and enabling the use of multiple medical datasets."
OG_ENG_ESSAY,og-eng-34,"There are 700,000 Rheumatoid Arthritis (RA) patients in Japan, and the number of patients is increased by 30,000 annually. The early detection and appropriate treatment according to the progression of RA are effective to improve the patient's prognosis. The modified Total Sharp (mTS) score is widely used for the progression evaluation of Rheumatoid Arthritis. The mTS score assessments on hand or foot X-ray image is required several times a year, and it takes very long time. The automatic mTS score calculation system is required. This paper proposes the finger joint detection method and the mTS score estimation method using support vector machine. Experimental results on 45 RA patient's X-ray images showed that the proposed method detects finger joints with accuracy of 81.4 %, and estimated the erosion and JSN score with accuracy of 50.9, 64.3 %, respectively."
OG_ENG_ESSAY,og-eng-35,"A common problem when trying to apply data mining techniques to improve educational systems is the disconnection between those who have the expertise (e.g. Universities) and those who have access to the data (e.g. Small companies). Bringing expertise into educational in-production systems is complicated because companies are reluctant to invest a lot of effort into integrating new technology that they do not fully trust, while the technology cannot prove its worth without access to real, valid data. In this paper we explore the requirements that machine learning systems have to be applied to specific learning problems (sequencing and performance prediction), and then propose a minimally invasive protocol for sequencing (based on web services) to easily integrate Learning Analytics Services into e-learning systems."
OG_ENG_ESSAY,og-eng-36,"Web filtering based on user's demand has witnessed a booming interest due to the development of Internet In the research community the dominant approach to this problem is based on machine learning algorithms. Web filtering is an inductive process which automatically builds a filter by learning from a set of pre-assigned document and the description of user's interest, and then uses it to assign unfiltered Web pages. This survey compares four main machine learning algorithms (decision tree, rule induction, Bayesian algorithm and support vector machines) on Chinese web pages set of their filtering effectiveness and computer resources consumed, focusing on the influence of feature set size and training set size. It induces that support vector machines earn high score in Chinese Web filtering applications."
OG_ENG_ESSAY,og-eng-37,"Timing closure is essential in SoC physical design. In this paper, machine learning models for timing prediction after floorplan are established. In the models, the features are selected and abstracted by analyzing these parameters from gate-level netlist, constraint files, and floorplan files. The classic machine learning algorithms, such as neural network, support vector machine (SVM), and ensemble machine learning, are explored. The corresponding regression models are applied to predict the timing of SoC. The testcase constructed by open source IP core is used to verify the proposed idea. The results show that the hybrid ensemble learning model has the best prediction performance among various learning models evaluated in this paper."
OG_ENG_ESSAY,og-eng-38,"This paper addresses the recent trends in machine learning methods for the automatic classification of remote sensing (RS) images. In particular, we focus on two new paradigms: semisupervised and active learning. These two paradigms allow one to address classification problems in the critical conditions where the available labeled training samples are limited. These operational conditions are very usual in RS problems, due to the high cost and time associated with the collection of labeled samples. Semisupervised and active learning techniques allow one to enrich the initial training set information and to improve classification accuracy by exploiting unlabeled samples or requiring additional labeling phases from the user, respectively. The two aforementioned strategies are theoretically and experimentally analyzed considering SVM-based techniques in order to highlight advantages and disadvantages of both strategies."
OG_ENG_ESSAY,og-eng-39,"In order to examine malicious activity that occurs in a network or a system, intrusion detection system is used. Intrusion Detection is software or a device that scans a system or a network for a distrustful activity. Due to the growing connectivity between computers, intrusion detection becomes vital to perform network security. Various machine learning techniques and statistical methodologies have been used to build different types of Intrusion Detection Systems to protect the networks. Performance of an Intrusion Detection is mainly depends on accuracy. Accuracy for Intrusion detection must be enhanced to reduce false alarms and to increase the detection rate. In order to improve the performance, different techniques have been used in recent works. Analyzing huge network traffic data is the main work of intrusion detection system. A well-organized classification methodology is required to overcome this issue. This issue is taken in proposed approach. Machine learning techniques like Support Vector Machine (SVM) and Naïve Bayes are applied. These techniques are well-known to solve the classification problems. For evaluation of intrusion detection system, NSL- KDD knowledge discovery Dataset is taken. The outcomes show that SVM works better than Naïve Bayes. To perform comparative analysis, effective classification methods like Support Vector Machine and Naive Bayes are taken, their accuracy and misclassification rate get calculated."
OG_ENG_ESSAY,og-eng-40,"In the past years, mobile devices were limited to textual content. However, the current generation has started to access richer multimedia content such as video, increasing the diversity of devices accessing the Web. Then, a problem arises as some of those devices characteristics like memory capacity or screen resolution turn the access to a content restricted. The present work considers the use of machine learning techniques as part of a dynamic video adaptation process, comparing the results from two of the most used approaches for data analysis, Multilayer Perceptron and Bayesian Inference, as part of a Decision Engine, analyzing data like device's capabilities, user's preferences and network condition in order to take the most appropriate way to adapt a video stream."
OG_ENG_ESSAY,og-eng-41,"In practice, there are many imbalanced data classification problems, for example, spam filtering, credit card fraud detection and software defect prediction etc. it is important in theory as well as in application for investigating the problem of imbalanced data classification. In order to deal with this problem, based on extreme learning machine autoencoder, this paper proposed an approach for addressing the problem of binary imbalanced data classification. The proposed method includes 3 steps. (1) the positive instances are used as seeds, new samples are generated for increasing the number of positive instances by extreme learning machine autoencoder, the generated new samples are similar with the positive instances but not same. (2) step (1) is repeated several times, and a balanced data set is obtained. (3) a classifier is trained with the balanced data set and used to classify unseen samples. The experimental results demonstrate that the proposed approach is feasible and effective."
OG_ENG_ESSAY,og-eng-42,"Manufacturing industry is facing major challenges to meet customer requirements, which are constantly changing. Therefore, products have to be manufactured with efficient processes, minimal interruptions, and low resource consumptions. To achieve this goal, huge amounts of data generated by industrial equipment needs to be managed and analyzed by modern technologies. Since the big data era in manufacturing industry is still at an early stage, there is a need for a reference architecture that incorporates big data and machine learning technologies and aligns with the Industrie 4.0 standards and requirements. In this paper, requirements for designing a scalable analytics platform for industrial data are derived from Industrie 4.0 standards and literature. Based on these requirements, a reference big data architecture for industrial machine learning applications is proposed and compared to related works. Finally, the proposed architecture has been implemented in the Lab Big Data at the SmartFactoryOWL and its scalability and performance have been evaluated on parallel computation of an industrial PCA model. The results show that the proposed architecture is linearly scalable and adaptable to machine learning use cases and will help to improve the industrial automation processes in production systems."
OG_ENG_ESSAY,og-eng-43,"Compromising wavelet kernel and multi-class least squares support vector machines, this article put forward to a fault diagnosis model based on multi-class wavelet support vector machine. The model heightens auto-adaptive model classification ability by adjustment of scale parameter of wavelet kernel, and make use of remarkable learning ability and generalization ability of small sample of vector machines to improve speed and effectiveness of fault diagnosis. And taking fault diagnosis in heat-recycling system of thermal power plant as an example to analysis."
OG_ENG_ESSAY,og-eng-44,"The Euclidean distance is usually chosen as the similarity measure in the conventional similarity metrics, which usually relates to all attributes. The smaller the distance is, the greater the similarity is. All the features of each vector have different functions in describing samples. So we can decide on the different functions of every feature by using feature weight learning, that is, introduce feature weight parameters to the distance formula. Feature weight learning can be viewed as a linear transformation for a set of points in the Euclidean space. The numerical experiments applied in K-means clustering prove the validity of this learning algorithm."
OG_ENG_ESSAY,og-eng-45,"This thesis elaborated the concept, significance and main strategy of machine learning as well as the basic structure of machine learning system. By combining several basic ideas of main strategies, great effort are laid on introducing several machine learning methods, such as Rote learning, Explanation-based learning, Learning from instruction, Learning by deduction, Learning by analogy and Inductive learning, etc. Meanwhile, comparison and analysis are made upon their respective advantages and limitations. At the end of the article, it proposes the research objective of machine learning and points out its development trend.Machine learning is a fundamental way that enable the computer to have the intelligence ; Its application which had been used mainly the method of induction and the synthesis, rather than the deduction has already reached many fields of Artificial Intelligence."
OG_ENG_ESSAY,og-eng-46,"As the expansion of the standard Support Vector Machine, compared with the traditional standard Support Vector Machine, the Least Squares Support Vector Machine loses the sparseness of standard Support Vector Machine, which would affect the efficiency of the second study. Aimed at the above puzzle, the article proposed an improved Least Squares Support Vector Machine incremental learning method, using self-adaptive methods to prune the sample, according to the performance of the classifier which each training has been to set the pruning threshold and the increment size of the sample. If you get a good performance of classifier, pruning threshold and sample increment is big, the other hand, if you get a poor performance of classifier, pruning threshold and sample increment is small, resulting in improved efficiency of Least Squares Support Vector Machine training to solve the sparse problem. The simulation experiment results verify the proposed algorithm is feasible."
OG_ENG_ESSAY,og-eng-47,"The Machine Type Communication Devices (MTCDs) are usually based on Internet Protocol (IP), which can cause billions of connected objects to be part of the Internet. The enormous amount of data coming from these devices are quite heterogeneous in nature, which can lead to security issues, such as injection attacks, ballot stuffing, and bad mouthing. Consequently, this work considers machine learning trust evaluation as an effective and accurate option for solving the issues associate with security threats. In this paper, a comparative analysis is carried out with five different machine learning approaches: Naive Bayes (NB), Decision Tree (DT), Linear and Radial Support Vector Machine (SVM), KNearest Neighbor (KNN), and Random Forest (RF). As a critical element of the research, the recommendations consider different Machine-to-Machine (M2M) communication nodes with regard to their ability to identify malicious and honest information. To validate the performances of these models, two trust computation measures were used: Receiver Operating Characteristics (ROCs), Precision and Recall. The malicious data was formulated in Matlab. A scenario was created where 50% of the information were modified to be malicious. The malicious nodes were varied in the ranges of 10%, 20%, 30%, 40%, and the results were carefully analyzed."
OG_ENG_ESSAY,og-eng-48,"Machine Translation(MT) is a part of Natural Language Processing(NLP). It is the method of translating Source Language(SL) text into Target Language(TL). The gap between computer programmer and linguist can be resolved with this system. The problems like lexical ambiguity, part of speech tagging, syntactic and structural ambiguity, synonym etc will arise while developing such systems. To develop a proper bilingual machine translation system for two natural languages is a challenging and demanding task for researchers. It is required to analyze the information as well as technology behind every natural language translation. This work represents the various approaches with current trends of machine translation system. In recent trends various machine learning approaches have been developed to tackle the above said problems. Most recently neural machine translation attain very good result by using different deep neural network techniques and machine learning algorithms."
OG_ENG_ESSAY,og-eng-49,"1960s civil rights and racial justice activists tried to warn us about our technological ways, but we didn't hear them talk. The so-called wizards who stayed up late ignored or dismissed black voices, calling out from street corners to pulpits, union halls to the corridors of Congress. Instead, the men who took the first giant leaps towards conceiving and building our earliest ""thinking"" and ""learning"" machines aligned themselves with industry, government and their elite science and engineering institutions. Together, they conspired to make those fighting for racial justice the problem that their new computing machines would be designed to solve. And solve that problem they did, through color-coded, automated, and algorithmically-driven indignities and inumahities that thrive to this day. But what if yesterday's technological elite had listened to those Other voices? What if they had let them into their conversations, their classrooms, their labs, boardrooms and government task forces to help determine what new tools to build, how to build them and - most importantly - how to deploy them? What might our world look like today if the advocates for racial justice had been given the chance to frame the day's most preeminent technological question for the world and ask, ""Computerize the Race Problem?"" Better yet, what might our AI-driven future look like if we ask ourselves this question today?"
OG_ENG_ESSAY,og-eng-50,"Computer mice have their displacement sensors in various locations (center, front, and rear). However, there has been little research into the effects of sensor position or on engineering approaches to exploit it. This paper first discusses the mechanisms via which sensor position affects mouse movement and reports the results from a study of a pointing task in which the sensor position was systematically varied. Placing the sensor in the center turned out to be the best compromise: improvements over front and rear were in the 11-14% range for throughput and 20--23% for path deviation. However, users varied in their personal optima. Accordingly, variable-sensor-position mice are then presented, with a demonstration that high accuracy can be achieved with two static optical sensors. A virtual sensor model is described that allows software-side repositioning of the sensor. Individual-specific calibration should yield an added 4% improvement in throughput over the default center position."
OG_ENG_ESSAY,og-eng-51,"Teaching parallel and distributed computing (PDC) concepts is an ongoing and pressing concern for many undergraduate educators. The ACM/IEEE CS Joint Task Force on Computing Curricula (CS2013) recommends 15 hours of PDC education in the undergraduate curriculum. Most recently, the 2019 ABET Criteria for Accrediting Computer Science requires coverage of PDC topics. For faculty who are unfamiliar with PDC, the prospect of incorporating parallel computing into their courses can seem very daunting. For example, should PDC concepts be covered in a single required course (perhaps computer systems) or be scattered throughout different courses in the undergraduate curriculum? What languages are the best/easiest for students to learn PDC? How much revision is truly needed? This Birds of a Feather session provides a platform for computing educators to discuss the common challenges they face when attempting to incorporate PDC into their curricula and share potential solutions. Chiefly, the organizers are interested in identifying ""gap areas"" that hinder a faculty member's ability to integrate PDC into their undergraduate courses. The multiple viewpoints and expertise provided by the BOF leaders should lead to lively discourse and enable experienced faculty to share their strategies with those beginning to add PDC across their curricula. We anticipate that this session will be of interest to all CS faculty looking to integrate PDC into their courses and curricula."
OG_ENG_ESSAY,og-eng-52,"Computing Education Research (CER) implicitly assumes that CS undergraduate students have no barrier to access a learning platform or software package. The assumption that ""everyone has access to and uses a device"" endangers the validity of CER studies by overlooking a critical element in what students access and how students use computing resources. First, in this work, we explicitly investigate undergraduate student usage (how often) of computing resources (laptops and computer labs) on a university campus. Furthermore, we investigate whether CS student usage of laptops and computer labs are factors of success in computing education to close a crucial feedback loop for CER and CS educators. Second, previous studies studying student's technology equipped used qualitative surveys, and lacked a systematic and continuous view of the student population. In this work, we address this shortcoming by developing a method to use operational data sets from the wireless networks and computer labs on campus. Operational data sets provide a systematic and continuous coverage of all students, that is, a student's absence of usage becomes a data point instead of a missing point. Results indicate that the use of equipment levels may be lower than national average. Nevertheless, there exists a positive correlation between higher frequency of laptop usage and success in computing education."
OG_ENG_ESSAY,og-eng-53,"The purpose of this study were to 1) study the current problem, requirement and condition of learning environment of computer teaching, 2)develop a model of creativity based learning for computer teaching, 3) evaluate the results of using a model of creativity based learning, and 4) assess the learning achievement from using creativity based learning model. The respondents of this study are 59 undergraduate students from computer program. The research instruments include course syllabus, pretest and posttest questionnaire, and paper test of creativity design based on four parts of rubric scale. Data were analyzed by descriptive statistical mean, standard deviation, and t-test. The results found that 59 sample agreed with a model of creativity-based learning, and the students were able to learn better when creativity -based learning apply with information media and technology, creativity and innovation and collaboration, teamwork and leadership. The learning achievement from using creativity -based learning was higher than pre-test at the 0.05 level of significance. The students were satisfied with using creativity-based learning to show the ideas of creating and learning by themselves."
OG_ENG_ESSAY,og-eng-54,"This paper expands on knowledge of computing identity by building on what is known about prior identity models in science and mathematics education. The model theorizes three primary sub-constructs that contribute to the development of a computing identity: belief in one's performance/competence, interest, and recognition in computing. Drawing on data from a nationally representative survey of more than 1,700 college students at 22 colleges and universities, the study tested the alignment of the theorized model to the measures on the survey. Confirmatory Factor Analysis was used to validate whether the appropriate measures loaded on the three separate sub-constructs. Criterion-related validity was also established by testing whether the computing identity measures predicted the choice of a computer science career. The results reveal that a computing identity proxy based on the theorized measures was a highly significant predictor of students' computer science and information technology career choice (p < 0.0001). In addition, this work also established criterion-related validity by showing gender differences that had been found by prior work in computing. Finally, the theorized measures were found to be reliable and internally consistent. The educational understanding of computing identities may provide an important tool to help researchers and practitioners improve student persistence in computer science."
OG_ENG_ESSAY,og-eng-55,"We report on a curricular experiment at Stanford University focused on teaching computer ethics. After nearly a year of preparation, we launched a new course at the intersection of ethics, public policy, and technology that deeply marries the humanities, social sciences, and computer science. While the teaching of computer ethics courses dates back decades, such courses are often taught by a (single) CS faculty member without significant training in ethics, do not include a policy component, and are meant for CS students. By contrast, we take a deeply multidisciplinary approach, where three faculty instructors, from philosophy, political science, and CS, each bring their respective lens to four related course modules: algorithmic decision-making, data privacy and civil liberties, AI and autonomous systems, and the power of platform companies. Panels of guest speakers drawn from academia, industry, civil society, and government provide a practitioner's view of the topics addressed. Additionally, custom case studies were developed under the direction of the course staff. These materials (videos of the speaker panels and the case studies) are freely available for use by the broader community. We report on the details of the course structure, including how multiple disciplines are integrated throughout the course, including lectures, discussions, and assignments. We discuss aspects of the course that worked well as well as challenges in making the course broadly accessible (beyond just CS majors). Importantly, we also include a discussion of students' response to the course, showing that a deeply multidisciplinary approach resonates strongly with them."
OG_ENG_ESSAY,og-eng-56,"As the digital economy grows, so does the demand for technology-capable workers who have both computing skills and domain expertise. Growing such a workforce is critical to ensuring the nation's competitiveness, according to a recent National Science Board publication. To address this need, faculty from the Colleges of Engineering and Social Sciences at San Jóse State University worked together to create the Applied Computing for Behavioral and Social Sciences minor degree. The minor targets students in majors such as Psychology and Economics, which have a more diverse student population than that of Computer Science or Engineering. The minor, designed with industry input, includes a four-course sequence that focuses on Python and R and includes topics such as data structures, algorithms, data cleaning and management, and data analysis. Our cohort-based program was built specifically for social science students using social science content, helping to foster a sense of community and belongingness among students. The first full cohort of 26 students graduated in Spring 2019, 48% of whom were female and 23% of whom were underrepresented minorities. Our approach of embedding computing education into the social sciences demonstrates a promising model of broadening participation in computing and meeting the nation's increasing demand for interdisciplinary computing workers in the digital age."
OG_ENG_ESSAY,og-eng-57,"Advances in nanosatellite technology and a declining cost of access to space have fostered an emergence of large constellations of sensor-equipped satellites in low-Earth orbit. Many of these satellite systems operate under a ""bent-pipe"" architecture, in which ground stations send commands to orbit and satellites reply with raw data. In this work, we observe that a bent-pipe architecture for Earth-observing satellites breaks down as constellation population increases. Communication is limited by the physical configuration and constraints of the system over time, such as ground station location, nanosatellite antenna size, and energy harvested on orbit. We show quantitatively that nanosatellite constellation capabilities are determined by physical system constraints."
OG_ENG_ESSAY,og-eng-58,"A Lifetime Service award suggests many years of being involved in Computer Science Education. Over these years I have gained numerous active caring insights to promote support and guide Computer Science Educators. Being a successful CS educator does not just involve gaining a qualification and teaching classes. There are so many exciting and rewarding aspects to this career that enhance the classroom experience for all and also add to the promotion and excitement of the discipline. These insights are gathered from over 40 years of teaching and supervising students, 25 years as a Chair of Department and some exciting and innovative research projects. From the villages in the high Andes to local diversity issues, from coding in binary and hex to apps on smart phones the revolution of the digital age has been a major part of my journey in computer science education."
OG_ENG_ESSAY,og-eng-59,"What can be Computed is a textbook for a course that covers Decidable Languages, Computable Functions, Recursively Enumerable Languages, Turing Machines, Reductions, Regular Languages, P, NP, and NPcompleteness. Since there are already many books with these topics in them, the question is:"
OG_ENG_ESSAY,og-eng-60,"Contemporary computing devices contain a concoction of numerous hazardous materials. Though users are more or less protected from these substances, recycling and landfilling reintroduce them to the biosphere where they may be ingested by people. This paper calls on HCI researchers to consider these corporal interactions with computers and critiques HCI's existing responses to the e-waste problem. We propose that whether one would consider eating a particular electronic component offers a surprisingly useful heuristic for whether we ought to be producing it on mass with vanishingly short lifespans. We hypothesize that the adoption of this heuristic might affect user behaviour and present a diet plan for users who wish to take responsibility for their own e-waste by eating it. Finally we propose an alternative direction for HCI researchers to design and advocate for those affected by the material properties of e-waste."
OG_ENG_ESSAY,og-eng-61,"In this column, we introduce our Model AI Assignment, A Module on Ethical Thinking about Autonomous Vehicles in an AI Course, and more broadly introduce a conversation on ethics education in AI education."
OG_ENG_ESSAY,og-eng-62,"Humans and AI systems are usually portrayed as separate systems that we need to align in values and goals. However, there is a great deal of AI technology found in non-autonomous systems that are used as cognitive tools by humans. Under the extended mind thesis, the functional contributions of these tools become as essential to our cognition as our brains. But AI can take cognitive extension towards totally new capabilities, posing new philosophical, ethical and technical challenges. To analyse these challenges better, we define and place AI extenders in a continuum between fully-externalized systems, loosely coupled with humans, and fully internalized processes, with operations ultimately performed by the brain, making the tool redundant. We dissect the landscape of cognitive capabilities that can foreseeably be extended by AI and examine their ethical implications.We suggest that cognitive extenders using AI be treated as distinct from other cognitive enhancers by all relevant stakeholders, including developers, policy makers, and human users."
OG_ENG_ESSAY,og-eng-63,"When we think about the values AI should have in order to make right decisions and avoid wrong ones, there's a large but hidden third category to consider: decisions that are not-wrong but also not-right. This is the grey space of judgment calls, and just having good values might not help as much as you'd think here. I'll use autonomous cars as my case study here, with lessons for broader AI: ethical dilemmas can arise in everyday scenarios such as lane positioning and navigation, not just in crazy crash scenarios. This is the space where one good value might conflict with another good value, and there's no ""right"" answer or even broad consensus on an answer; so, it's important to recognize the hard cases-which are potential limits-in the study of AI ethics."
OG_ENG_ESSAY,og-eng-64,"The ability of an AI agent to build mental models can open up pathways for manipulating and exploiting the human in the hopes of achieving some greater good. In fact, such behavior does not necessarily require any malicious intent but can rather be borne out of cooperative scenarios. It is also beyond the scope of misinterpretation of intents, as in the case of value alignment problems, and thus can be effectively engineered if desired (i.e. algorithms exist that can optimize such behavior not because models were misspecified but because they were misused). Such techniques pose several unresolved ethical and moral questions with regards to the design of autonomy. In this paper, we illustrate some of these issues in a teaming scenario and investigate how they are perceived by participants in a thought experiment. Finally, we end with a discussion on the moral implications of such behavior from the perspective of the doctor-patient relationship."
OG_ENG_ESSAY,og-eng-65,"It has become commonplace to assert that autonomous agents will have to be built to follow human rules of behavior--social norms and laws. But human laws and norms are complex and culturally varied systems; in many cases agents will have to learn the rules. This requires autonomous agents to have models of how human rule systems work so that they can make reliable predictions about rules. In this paper we contribute to the building of such models by analyzing an overlooked distinction between important rules and what we call silly rules -- rules with no discernible direct impact on welfare. We show that silly rules render a normative system both more robust and more adaptable in response to shocks to perceived stability. They make normativity more legible for humans, and can increase legibility for AI systems as well. For AI systems to integrate into human normative systems, we suggest, it may be important for them to have models that include representations of silly rules."
OG_ENG_ESSAY,og-eng-66,"My dissertation asks two fundamental questions: What are the risks of AI? And what should be done about them? My research goes beyond existential threats to humanity to consider seven dimensions of AI risk: military, political, economic, social, environmental, psychophysiological, and spiritual. I examine extant AI risk mitigation strategies and, finding them insufficient, use a democratic governance framework to propose alternatives. This paper outlines the project and introduces the risk dimensions."
OG_ENG_ESSAY,og-eng-67,"Open-access AI educational resources are vital to the quality of the AI education we offer. Avoiding the reinvention of wheels is especially important to us because of the special challenges of AI Education. AI could be said to be ""the really interesting miscellaneous pile of Computer Science"". While ""artificial"" is well-understood to encompass engineered artifacts, ""intelligence"" could be said to encompass any sufficiently difficult problem as would require an intelligent approach and yet does not fall neatly into established Computer Science subdisciplines. Thus AI consists of so many diverse topics that we would be hard- pressed to individually create quality learning experiences for each topic from scratch."
OG_ENG_ESSAY,og-eng-68,"We present an innovative methodology for studying and teaching the impacts of AI through a role-play game. The game serves two primary purposes: 1) training AI developers and AI policy professionals to reflect on and prepare for future social and ethical challenges related to AI and 2) exploring possible futures involving AI technology development, deployment, social impacts, and governance. While the game currently focuses on the inter-relations between short-, mid- and long-term impacts of AI, it has potential to be adapted for a broad range of scenarios, exploring in greater depths issues of AI policy research and affording training within organizations."
OG_ENG_ESSAY,og-eng-69,"Over the past few years, specialised online and offline press blossomed with articles about art made ""with"" Artificial Intelligence (AI) but the narrative is rapidly changing. In fact, in October 2018, the auction house Christie's sold an art piece allegedly made ""by"" an AI. We draw from philosophy of art and science arguing that AI as a technical object is always intertwined with human nature despite its level of autonomy. However, the use of creative autonomous agents has cultural and social implications in the way we experience art as creators as well as audience. Therefore, we highlight the importance of an interdisciplinary dialogue by promoting a culture of transparency of the technology used, awareness of the meaning of technology in our society and the value of creativity in our lives."
OG_ENG_ESSAY,og-eng-70,"Artificial Intelligence and robotics are rapidly moving into healthcare, playing key roles in specific medical functions, including diagnosis and clinical treatment. Much of the focus in the technology development has been on human-machine interactions, leading to a host of related technology-centric questions. In this paper, we focus instead on the impact of these technologies on human-human interactions and relationships within the healthcare domain. In particular, we argue that trust plays a central role for relationships in the healthcare domain, and the introduction of healthcare AI can potentially have significant impacts on those relations of trust."
OG_ENG_ESSAY,og-eng-71,"There has been a growing awareness of Artificial Intelligence's (AI) inherent biases, limitations, and the challenges in overcoming them. As AI is integrated into to all things technical, there is a valid concern over its diversity, inclusiveness, and accessibility. However, questions such as what does it mean for AI to be inclusive, why is inclusive AI required, and how can it be achieved, largely remain unanswered. In this paper, we highlight these issues to initiate discussions on what it means for AI to be inclusive."
OG_ENG_ESSAY,og-eng-72,"Since its codified genesis in the 18th century, ballet training has largely been unchanged: it relies on the word of mouth expertise passed down generation to generation and in tools that do not adequately support both dancers and teachers. Moreover, top-tier training is only found in a few locations around the world and comes at an exceptional price. In this context, artificial intelligence (AI)-based video tools might represent an affordable and non-invasive alternative: it would allow dancers and teachers to self-assess as well as enable skilled dance teachers to connect with a wider audience. In my research, I study how to design and evaluate AI-based tools to improve ballet performance for dancers and teachers."
OG_ENG_ESSAY,og-eng-73,"Implicit in any rhetorical interaction-between humans or between humans and machines-are ethical codes that shape the rhetorical context, the social situation in which communication happens and also the engine that drives communicative interaction. Such implicit codes are usually invisible to AI writing systems because the social factors shaping communication (the why and how of language, not the what) are not usually explicitly evident in databases the systems use to produce discourse. Can AI writing systems learn to learn rhetorical context, particularly the implicit codes for communication ethics? We see evidence that some systems do address issues of rhetorical context, at least in rudimentary ways."
OG_ENG_ESSAY,og-eng-74,"New technologies, particularly those which are deployed rapidly across sectors, or which have to operate in competitive conditions, can disrupt previously stable technology governance regimes. This leads to a precarious need to balance caution against performance while exploring the resulting 'safe operating space'. This paper will argue that Artificial Intelligence is one such critical technology, the responsible deployment of which is likely to prove especially complex, because even narrow AI applications often involve networked (tightly coupled, opaque) systems operating in complex or competitive environments. This ensures such systems are prone to 'normal accident'-type failures which can cascade rapidly, and are hard to contain or even detect in time."
OG_ENG_ESSAY,og-eng-75,"Estimation, planning, control, and learning are giving us robots that can generate good behavior given a specified objective and set of constraints. What I care about is how humans enter this behavior generation picture, and study two complementary challenges: 1) how to optimize behavior when the robot is not acting in isolation, but needs to coordinate or collaborate with people; and 2) what to optimize in order to get the behavior we want. My work has traditionally focused on the former, but more recently I have been casting the latter as a human-robot collaboration problem as well (where the human is the end-user, or even the robotics engineer building the system)."
OG_ENG_ESSAY,og-eng-76,"Artificial Intelligence (AI) ethics is by no means a new discipline; thinkers like Asimov and Philip K Dick laid the foundations of this field decades ago. Both then and today, popular dilemmas in AI ethics largely focus on artificial consciousness, artificial general intelligence, autonomous weapons, and some version of the trolley problem. While these thought experiments may prove useful in the future, modern AI applications that are in use today raise ethical dilemmas that require urgent resolution. Public outcry in response to AI in health care, criminal justice, and employment highlight the urgency of the matter. These real and imminent ethical challenges posed by AI form the basis of my dissertation research. In particular, I focus on domains where AI is necessary or inevitable -- such as kidney exchange and medical image classification -- and ethical challenges are unavoidable."
OG_ENG_ESSAY,og-eng-77,"The last few years have seen a huge growth in the capabilities and applications of Artificial Intelligence (AI) and autonomous systems. Hardly a day goes by without news about technological advances and the societal impact of the use of AI. AI is changing the way we work, live and solve challenges. For example, it can improve healthcare, protect elephants from poachers, and work out how broadband should be distributed."
OG_ENG_ESSAY,og-eng-78,"Certainty around the regulatory environment is crucial to facilitate responsible AI innovation and its social acceptance. However, the existing legal liability system is inapt to assign responsibility where a potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations of AI and/or the harms they may trigger are not foreseeable in the legal sense. The unpredictability of how courts would handle such cases makes the risks involved in the investment and use of AI incalculable, creating an environment that is not conducive to innovation and may deprive society of some benefits AI could provide. To tackle this problem, we propose to draw insights from financial regulatory best-practices and establish a system of AI guarantee schemes."
OG_ENG_ESSAY,og-eng-79,"Artificial intelligence and machine learning (AI/ML) are some of the newest trends to hit the software industry, compelling organizations to evolve their development processes to deliver novel products to their customers. In this talk, I describe a study in which we learned how Microsoft software teams develop AI/ML-based applications using a nine-stage AI workflow process informed by prior experiences developing early AI applications (e.g. search and NLP) and data science tools (e.g. application telemetry and bug reporting). Adapting this workflow into their pre-existing, well-evolved, Agile-like software engineering processes and job roles has resulted in a number of engineering challenges unique to the AI/ML domain, some universal to all teams, but others related to the amount of prior AI/ML experience and education the teams have. I tell you about some challenges and the solutions that teams have come up with. The lessons that Microsoft has learned can help other organizations embarking on their own path towards AI and ML."
OG_ENG_ESSAY,og-eng-80,"Remotely Unattended Installation / Uninstallation of Software is a collection of tools that allow the network administrator, sitting at his or her desk, to manage a network of remote PCs and accomplish the tasks like, support of daily IT operations and management task; whether your job is deploying software, Installation of softwares, collecting information from remote computers. These tasks include the installation of new applications and updates, remote control or monitoring of network workstations, keeping an inventory of software and hardware installed on those systems, and many other functions that will be described in detail throughout the manual. Administrator can send installable files of software to the number of client PC if client PCs requested that software. Administrator can get all the information of client PCs to its terminal like hardware configuration, software configuration, memory status, port status of client PCs."
OG_ENG_ESSAY,og-eng-81,"Software needs to grow up and become responsible for itself and its own future by participating in its own installation and customization, maintaining its own health, and adapting itself to new circumstances, new users, and new uses. To create such software will require us to change some of our underlying assumptions about how we write programs. A promising approach seems to be to separate software that does the work (allopoietic)from software that keeps the system alive (autopoietic)."
OG_ENG_ESSAY,og-eng-82,"In lots of software projects unfortunately an architectural erosion happens over time. Modules which were independent, become connected, plug-ins finally depend on each other, and in general the architecture gets violated more and more. In this paper we will discuss how to avoid such architecture- and design-erosion and how an already eroded system can be fixed again. We will look at three different level of static analysis and examine architectural analysis in detail. Also typical use cases for architectural analysis are examined, followed by a collection of requirements for powerful tool support. The eclipse platform serves as case study and we look if, and how far architectural erosion happened there and if it can be fixed. Finally we discuss pros and cons of architectural analysis and conclude with an out view."
OG_ENG_ESSAY,og-eng-83,"This paper deals with the problem of reliability in a hardware/software system. More specifically it deals with the strategy used to achieve reliability in a particular hardware/software system built by the author and his colleagues at Carnegie-Mellon University. Rather than dealing with the myriad details of the reliability aspects of this systems, the paper focuses on the design philosophy which aims at keeping the system operational even though the underlying hardware may be malfunctioning. This philosophy is essentially an extension of the 'modular' programming methodology, advocated by Parnas and others, to include dynamic error detection and recovery."
OG_ENG_ESSAY,og-eng-84,"Every discipline, e.g. medicine and engineering, has its own vocabulary to describe situations and tools. This dedicated language is important, because it allows for being specific, detailed and precise. On the other hand, this language, specific to each discipline, becomes a barrier for communication across disciplines. International software measurement standards are examples of such language. The standards are documents that provide definitions of terms used and describe processes specific to the discipline of software measurement. However, one major problem we have observed is that standards are difficult to read and to understand; even for the stakeholders that they are intended for."
OG_ENG_ESSAY,og-eng-85,"Agile methodology uses the incremental and iterative method and is commonly utilized in the Pakistan's industry projects as they can accommodate changes in requirements. Product distribution is accomplished by using small iterations/repetitions, but guaranteeing the quality of the product is important and crucial part as well as it is a tough task. Quality should be assured of the product that is developed using agile methodology. The study centers on the five key parts of software testing, explicitly software testing methods, software testing metrics, practices and techniques, testing standards, automated testing tools, and testing education & training. Grounded on survey outcomes, research paper evaluates the implementation of existing practices in the software testing, provide some recommendations and observations for the software testing future in Pakistan IT industry & also suggested the solution that how quality is assured in agile software development using different factors."
OG_ENG_ESSAY,og-eng-86,"No two flight missions are alike, hence, development and on-orbit software costs are high. Software portability and adaptability across hardware platforms and operating systems has been minimal at best. Standard interfaces across applications and/or common applications are almost non-existent. To reduce flight software costs, these issues must be addressed. This presentation describes how the Flight Software Branch at Goddard Space Flight Center has architected a solution to these problems."
OG_ENG_ESSAY,og-eng-87,"JaamSim is a free, open-source simulation package written in the Java programming language. It is a general-purpose dynamic simulator that supports discrete-event logic, continuous variables, and agent-based model design. A modern graphical user interface is provided that is comparable to commercial software, including drag-and-drop model building, an Input Editor, Output Viewer, and 3D graphics. Users are able to create their own palettes of high-level objects using standard Java and modern programming tools such as Eclipse. If you are writing hundreds or thousands of lines of code in a proprietary programming language provided with your commercial software, you would be far better off to write your code in Java and use JaamSim."
OG_ENG_ESSAY,og-eng-88,"This study addresses the problem of cost estimation in the context of software evolution by building a set of quantitative models and assessing their predictive power. The models aim at capturing the relationship between effort, productivity and a suite of metrics of software evolution extracted from empirical data sets."
OG_ENG_ESSAY,og-eng-89,"The Software Test Program (STP) is a cooperation between Motorola and the Center for Informatics of the Federal University of Pernambuco. It has been conceived with inspiration on the Medical Residency, adjusted to the software development practice. A Software Residency includes the formal teaching of the relevant concepts and deep practice, with specialization on some specific subject; here the focus is on software testing. The STP has been of great benefit to all parties involved."
OG_ENG_ESSAY,og-eng-90,"Software Engineering courses are essential for undergraduates to achieve a smooth transition from higher education to a career. However, many of these courses encounter complications that forbid them from meeting their goals such as: Real products and customers, project duration, software sophistication and more. At Chico State, we have implemented the Tech Startup Model in which the Software Engineering students partner with entrepreneurship students to allow for more collaboration and the creation of a customer-employee relation to address some of these issues. This model utilizes both Lean Startup as well as Agile Development to continuously test a student's ability to adapt to the customer's needs. The data accumulated from the past couple of semesters allowed us to analyze student behavior when exposed to the Tech Startup Model as opposed to other methods tested from previous semesters."
OG_ENG_ESSAY,og-eng-91,"Mining software engineering data has emerged as a successful research direction over the past decade. In this position paper, we advocate Software Intelligence (SI) as the future of mining software engineering data, within modern software engineering research, practice, and education. We coin the name SI as an inspiration from the Business Intelligence (BI) field, which offers concepts and techniques to improve business decision making by using fact-based support systems. Similarly, SI offers software practitioners (not just developers) up-to-date and pertinent information to support their daily decision-making processes. SI should support decision-making processes throughout the lifetime of a software system not just during its development phase."
OG_ENG_ESSAY,og-eng-92,"Despite its importance, Software Quality Engineering is longing to make its way into software lifecycle standards. This article analyses the recently published ISO/IEC 24748 systems and software lifecycle management guide against other standards and academic material, and proposes additions to make to the standard for it to properly promote and support software quality engineering."
OG_ENG_ESSAY,og-eng-93,"The global, local and firm economies depend on software in an ever increasing way. Writing effective software on a day to day basis, for a long time (decades, maybe) depends upon agility (as a process and timing) to have, as consequences, reliability, scalability and security, in times of software and systems as services.

The only way to achieve such tense and, in most cases, opposite goals, is to try and find business models (for software ops) that are continuously searching for a repeatable, scalable business models."
OG_ENG_ESSAY,og-eng-94,"This paper describes a demonstration of the product line engineering tool and framework Gears from BigLever software. Gears provides a single feature modeling language, a single variation point mechanism, and a single automated product configurator that are used to configure a product portfolio's shared engineering assets appropriately for each product in the portfolio. The result is an automated production line capability that can quickly produce any product in the portfolio from the same, single set of shared assets."
OG_ENG_ESSAY,og-eng-95,"Software Studio is a studio-based learning (SBL) curriculum designed to train students as professional software engineers. Traditional software engineering courses remain important, but suffer significant gaps in preparing students for professional engagement. We describe our curriculum model, highlight ways in which it fills these gaps, and offer a SWOT analysis. As practical guidance, we reflect on our missteps and successes in implementing Software Studio over the past five semesters. Finally, we suggest future directions for Software Studio"
OG_ENG_ESSAY,og-eng-96,"Despite its importance, Software Quality Engineering is longing to make its way into software lifecycle standards. This article analyses the recently published ISO/IEC 24748 systems and software lifecycle management guide against other standards and academic material, and proposes additions to make to the standard for it to properly promote and support software quality engineering."
OG_ENG_ESSAY,og-eng-97,"The global, local and firm economies depend on software in an ever increasing way. Writing effective software on a day to day basis, for a long time (decades, maybe) depends upon agility (as a process and timing) to have, as consequences, reliability, scalability and security, in times of software and systems as services."
OG_ENG_ESSAY,og-eng-98,"Engineering research differs greatly, both in its aims and in its methods, from traditional ""scientific"" research. While Sciences deal with the study of existing objects and phenomena, be it physically, metaphysically or conceptually, Engineering is based on how to do things, how to create new objects. For this reason, ""scientific"" research methods are not always directly applicable to research problems of an engineering nature.In the present article, we concentrate on the problems and research methods of a specific branch of engineering: Software Engineering, discussing, on the one hand, the nature of the method in this field while and, on the other, the similarity of the methods of research in Software Engineering and those of software development."
OG_ENG_ESSAY,og-eng-99,"Software Studio is a studio-based learning (SBL) curriculum designed to train students as professional software engineers. Traditional software engineering courses remain important, but suffer significant gaps in preparing students for professional engagement. We describe our curriculum model, highlight ways in which it fills these gaps, and offer a SWOT analysis. As practical guidance, we reflect on our missteps and successes in implementing Software Studio over the past five semesters. Finally, we suggest future directions for Software Studio."