{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PlagirismFeatures.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"-vhaKsE8HLIA","colab_type":"code","colab":{}},"source":[" #Feature Engineering\n","\n","\n","  # Clean and pre-process the text data.\n","  # Define features for comparing the similarity of an answer text and a source text, and extract similarity features.\n","  # Select \"good\" features, by analyzing the correlations between different features.\n","  # Create train/test .csv/.txt files that hold the relevant features and class labels for train/test data points.\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"br4rcxI3X9Gk","colab_type":"code","colab":{}},"source":["# import libraries\n","import pandas as pd\n","import numpy as np\n","import os\n","import operator \n","import re"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3EFOE7YbBGAY","colab_type":"code","outputId":"29256597-3cc4-4eea-b4d9-f2c79e82f4f8","executionInfo":{"status":"ok","timestamp":1588720665856,"user_tz":240,"elapsed":4267,"user":{"displayName":"Lauren Kruse","photoUrl":"","userId":"09178474056200440391"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["#more libraries\n","\n","from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn import decomposition, ensemble\n","\n","import pandas, xgboost, numpy, textblob, string\n","from keras.preprocessing import text, sequence\n","from keras import layers, models, optimizers\n","import spacy\n","import nltk\n","nltk.download('vader_lexicon')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"FIjObYZaYBNj","colab_type":"code","colab":{}},"source":["# Have to mount my drive otherwise I cannot import data from drive\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o4XTniYQYEf9","colab_type":"code","colab":{}},"source":["csv_file = '/content/drive/My Drive/Colab Notebooks/CrossLanguagePlagirism/CombinedData.csv'\n","plagiarism_df = pd.read_csv(csv_file)\n","\n","# print out the first few rows of data info\n","plagiarism_df.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QuUXqnpRZ0Ad","colab_type":"text"},"source":["#Convert Categorical Data to discrete "]},{"cell_type":"markdown","metadata":{"id":"PmxuLdFWaMF9","colab_type":"text"},"source":["\n","\n","\n","*   0 = French\n","*  1= English\n","\n","\n","*   2 = Teng\n","*   -1 = OG English\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FE7OGDfPa9BF","colab_type":"text"},"source":["Binary class label\n","\n","\n","\n","*   0 = plagiarized\n","*   1 = not plagiarized\n","\n","\n","*   2 = partially plagiarized (might be for translated text)\n","*   Might want to keep things binary ie plagiarized v. not plagiarized\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"eMDZVsp1bnjN","colab_type":"code","colab":{}},"source":["# Read in a csv file and return a transformed dataframe\n","def numerical_dataframe(csv_file='/content/drive/My Drive/Colab Notebooks/CrossLanguagePlagirism/CombinedData.csv'):\n","    '''Reads in a csv file which is assumed to have `File`, `Category` and `Task` columns.\n","       This function does two things: \n","       1) converts `Category` column values to numerical values \n","       2) Adds a new, numerical `Class` label column.\n","       The `Class` column will label plagiarized answers as 1 and non-plagiarized as 0.\n","       Source texts have a special label, -1.\n","       :param csv_file: The directory for the file_information.csv file\n","       :return: A dataframe with numerical categories and a new `Class` label column'''\n","    \n","    plagiarism_df = pd.read_csv(csv_file)\n","    \n","    labels = {\n","        \"French\":0,\n","        \"English\":1, # for our purposes are we considering manual translation plagiarism?\n","        \"TEng\":2,\n","        \"OG_ENG_ESSAY\":-1\n","    }\n","    plagiarism_df['Category'] = plagiarism_df.Label.map(labels)\n","    plagiarism_df['Class'] = plagiarism_df['Category'].apply(lambda x: 0 if x == 0 else (-1 if x==-1 else 1))\n","    return plagiarism_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uUOWpVntgdu4","colab_type":"code","colab":{}},"source":["#informal testing, print out the results of a called function\n","# create new `transformed_df`\n","transformed_df = numerical_dataframe(csv_file ='/content/drive/My Drive/Colab Notebooks/CrossLanguagePlagirism/CombinedData.csv')\n","\n","# check work\n","# check that all categories of plagiarism have a class label = 1\n","transformed_df.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x0CO7cFQ_ng5","colab_type":"text"},"source":["# Helper Functions\n","\n","I borrowed some of these helper functions from elsewhere"]},{"cell_type":"code","metadata":{"id":"8FLd-gEZ7Hjx","colab_type":"code","colab":{}},"source":["# Add 'datatype' column that indicates if the record is original wiki answer as 0, training data 1, test data 2, onto \n","# the dataframe - uses stratified random sampling (with seed) to sample by task & plagiarism amount \n","\n","# Use function to label datatype for training 1 or test 2 \n","def create_datatype(df, train_value, test_value, datatype_var, compare_dfcolumn, operator_of_compare, value_of_compare,\n","                    sampling_number, sampling_seed):\n","    # Subsets dataframe by condition relating to statement built from:\n","    # 'compare_dfcolumn' 'operator_of_compare' 'value_of_compare'\n","    df_subset = df[operator_of_compare(df[compare_dfcolumn], value_of_compare)]\n","    df_subset = df_subset.drop(columns = [datatype_var])\n","    \n","    # Prints counts by task and compare_dfcolumn for subset df\n","    #print(\"\\nCounts by Task & \" + compare_dfcolumn + \":\\n\", df_subset.groupby(['Task', compare_dfcolumn]).size().reset_index(name=\"Counts\") )\n","    \n","    # Sets all datatype to value for training for df_subset\n","    df_subset.loc[:, datatype_var] = train_value\n","    \n","    # Performs stratified random sample of subset dataframe to create new df with subset values \n","    df_sampled = df_subset.groupby(['Category', compare_dfcolumn], group_keys=False).apply(lambda x: x.sample(min(len(x), sampling_number), random_state = sampling_seed))\n","    df_sampled = df_sampled.drop(columns = [datatype_var])\n","    # Sets all datatype to value for test_value for df_sampled\n","    df_sampled.loc[:, datatype_var] = test_value\n","    \n","    # Prints counts by compare_dfcolumn for selected sample\n","    #print(\"\\nCounts by \"+ compare_dfcolumn + \":\\n\", df_sampled.groupby([compare_dfcolumn]).size().reset_index(name=\"Counts\") )\n","    #print(\"\\nSampled DF:\\n\",df_sampled)\n","    \n","    # Labels all datatype_var column as train_value which will be overwritten to \n","    # test_value in next for loop for all test cases chosen with stratified sample\n","    for index in df_sampled.index: \n","        # Labels all datatype_var columns with test_value for straified test sample\n","        df_subset.loc[index, datatype_var] = test_value\n","\n","    #print(\"\\nSubset DF:\\n\",df_subset)\n","    # Adds test_value and train_value for all relevant data in main dataframe\n","    for index in df_subset.index:\n","        # Labels all datatype_var columns in df with train_value/test_value based upon \n","        # stratified test sample and subset of df\n","        df.loc[index, datatype_var] = df_subset.loc[index, datatype_var]\n","\n","    # returns nothing because dataframe df already altered "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDD6yjyk7Q4-","colab_type":"code","colab":{}},"source":["def train_test_dataframe(clean_df, random_seed=100):\n","    \n","    new_df = clean_df.copy()\n","\n","    # Initialize datatype as 0 initially for all records - after function 0 will remain only for original wiki answers\n","    new_df.loc[:,'Datatype'] = 0\n","\n","    # Creates test & training datatypes for plagiarized answers (1,2,3)\n","    create_datatype(new_df, 1, 2, 'Datatype', 'Category', operator.gt, 0, 1, random_seed)\n","\n","    # Creates test & training datatypes for NON-plagiarized answers (0)\n","    create_datatype(new_df, 1, 2, 'Datatype', 'Category', operator.eq, 0, 2, random_seed)\n","    \n","    # creating a dictionary of categorical:numerical mappings for plagiarsm categories\n","    mapping = {0:'orig', 1:'train', 2:'test'} \n","\n","    # traversing through dataframe and replacing categorical data\n","    new_df.Datatype = [mapping[item] for item in new_df.Datatype] \n","\n","    return new_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6LcTF5G6-4bK","colab_type":"code","colab":{}},"source":["# helper function for pre-processing text given a file\n","def process_file(file):\n","    # put text in all lower case letters \n","    all_text = file.read().lower()\n","\n","    # remove all non-alphanumeric chars\n","    all_text = re.sub(r\"[^a-zA-Z0-9]\", \" \", all_text)\n","    # remove newlines/tabs, etc. so it's easier to match phrases, later\n","    all_text = re.sub(r\"\\t\", \" \", all_text)\n","    all_text = re.sub(r\"\\n\", \" \", all_text)\n","    all_text = re.sub(\"  \", \" \", all_text)\n","    all_text = re.sub(\"   \", \" \", all_text)\n","    \n","    return all_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LEQ2mlII-7zu","colab_type":"code","colab":{}},"source":["def create_text_column(df, file_directory='data/'):\n","    '''Reads in the files, listed in a df and returns that df with an additional column, `Text`. \n","       :param df: A dataframe of file information including a column for `File`\n","       :param file_directory: the main directory where files are stored\n","       :return: A dataframe with processed text '''\n","   \n","    # create copy to modify\n","    text_df = df.copy()\n","    \n","    # store processed text\n","    text = []\n","    \n","    # for each file (row) in the df, read in the file \n","    for row_i in df.index:\n","        filename = df.iloc[row_i]['File']\n","        #print(filename)\n","        file_path = file_directory + filename\n","        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n","\n","            # standardize text using helper function\n","            file_text = process_file(file)\n","            # append processed text to list\n","            text.append(file_text)\n","    \n","    # add column to the copied dataframe\n","    text_df['Text'] = text\n","    \n","    return text_df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vt0g7G8VoJ7A","colab_type":"text"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"cSS9E3DuBDiO","colab_type":"code","colab":{}},"source":["text_df = transformed_df\n","text_df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QAObxeKg5jBa","colab_type":"text"},"source":["# Split into Training and Test Set"]},{"cell_type":"code","metadata":{"id":"A8RxFBAwKT7H","colab_type":"code","colab":{}},"source":["random_seed = 1  # can change; set for reproducibility\n","\n","\"\"\"\n","\n","\"\"\"\n","\n","\n","# create new df with Datatype (train, test, orig) column\n","# pass in `text_df` from above to create a complete dataframe, with all the information you need\n","complete_df = train_test_dataframe(text_df, random_seed=random_seed)\n","\n","# check results\n","complete_df.head()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FeeVzxu7LZJQ","colab_type":"text"},"source":["#Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"VaqYh7irLkq1","colab_type":"text"},"source":["List of plagirism features\n","\n","1.   containment (https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c412841_developing-a-corpus-of-plagiarised-short-answers/developing-a-corpus-of-plagiarised-short-answers.pdf)\n","2.   lcs(Longest Common Subsequence)\n","\n","3.   word embeddings(Elmo/Word2vec)\n","4.   Something for cross language similarity?(https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0126196)\n","\n","\n","\n","5.   Vader( would hypothesis that a machine translation might lose some of the original sentiment that a manual translation might strive to capture)\n","6.   Maybe look into a Semantic feature for cross language and cross translation analysis \n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WlpqKDBIT3o_","colab_type":"text"},"source":["# Containment implementation"]},{"cell_type":"code","metadata":{"id":"1FM0TX86ZPe1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wIuowMofZQA9","colab":{}},"source":["#TODO I need to get it so that MT and and Manual translations compare. what I do is have the ENG and \n","#TENG article IDS to match \n"," \n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","def calculate_containment(df, n, answer_filename):\n","\n","  def get_answer_and_source(df: pd.DataFrame, file_name:str) -> (str, str):\n","      #get category of the answered item\n","      task =df.loc[df['ID']== file_name, 'Category'].item()\n","      #task =df.loc[df['Article']== file_name, 'Category'].item()\n","      match = df[df['ID'].str.match(r'(\\w{7}-\\d*-\\d*-(eng|teng))')==True]  # This matchs rows in article column that has 7 char word AKA article, and either teng or eng at the end\n"," \n","      answer_text = df.loc[df['ID']==file_name, 'Text'].item()\n","      # get the source text\n","      source_text =df.loc[(df['ID']== task)& (df['Category']== -1), 'Text'].item()\n","       \n","      return answer_text, source_text\n","# As long as article match but Category is different I should be able to get a match of source to answer\n","\n","      #get both text\n","\n","  answer, source = get_answer_and_source(df, answer_filename)\n","\n","  vectorizer= CountVectorizer(analyzer='word', ngram_range=(n,n))\n","  result = vectorizer.fit_transform([answer, source])\n","  result_np = result.toarray()\n","  intersection_np= np.amin(result_np,axis=0)\n","\n","  return intersection_np.sum()/result_np[0].sum()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"msTuSbP9o3Fz","colab_type":"code","colab":{}},"source":["n= 3\n","\n","test_indices = range(5)\n","\n","category_vals = []\n","\n","containment_vals = []\n","\n","for i in test_indices:\n","  category_vals.append(complete_df.loc[i, 'Category'])\n","\n","  filename = complete_df.loc[i, 'ID']\n","  c = calculate_containment(complete_df,n,filename)\n","  containment_vals.append(c)\n","\n","print ('Óriginal category values: \\n', category_vals)\n","print()\n","print(str(n)+ '-gram containment values: \\n', containment_vals)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wAHJegwUCXNe","colab_type":"text"},"source":["# Vader implementation "]},{"cell_type":"code","metadata":{"id":"tAThUvGwRiN-","colab_type":"code","colab":{}},"source":["#TODO finish fixing \n","\n","#Sentiment Analyzer VADER\n","def nltk_sentiment(sentence):\n","    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","    nltk_sentiment = SentimentIntensityAnalyzer()\n","    score = nltk_sentiment.polarity_scores(sentence)\n","    return score\n","\n","nltk_results = [nltk_sentiment(row) for row in plagiarism_df[\"Text\"]]\n","results_df = pandas.DataFrame(nltk_results)\n","#plagiarism_df = plagiarism_df.join(results_df)\n","\n","plagiarism_df['neg']= plagiarism_df['Text'].apply(lambda x:nltk_results['neg'])\n","plagiarism_df['pos']= plagiarism_df['Text'].apply(lambda x:nltk_results['pos'])\n","plagiarism_df['compound']= plagiarism_df['Text'].apply(lambda x: nltk_results['compound'])\n","plagiarism_df['neu']= plagiarism_df['Text'].apply(lambda x: nltk_results['neu'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9936vnS3L0ng","colab_type":"text"},"source":["# Longest Common Subsequence\n","\n","--- The longest common subsequence is the longest string of words or letters that are the same between Source text and answer text. This values is normalized by dividng by the total number of words or letters in the answer text\n","\n"]},{"cell_type":"code","metadata":{"id":"V9nLUmuQNe1z","colab_type":"code","colab":{}},"source":["def lcs_norm_word(answer_text, source_text):\n","\n","  answer_split = answer_text.split()\n","  source_split = source_text.split()\n","\n","  row = len(answer_split)\n","  col = len(source_split)\n","\n","  matrix = np.zeros((row+1, col +1))\n","\n","  i = 1\n","\n","  for answer in answer_split:\n","    j = 1\n","\n","    for source in source_split:\n","      if answer==source:\n","        matrix[i][j] = matrix[i - 1][j -1]+ 1\n","\n","      else:\n","        matrix[i][j] = max(matrix[i][j -1], matrix[i - 1][j])\n","      j = j +1\n","    i = i + 1\n","  return matrix[row][col]/ row\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5S0NkorRHbf","colab_type":"code","colab":{}},"source":["A = \"i think pagerank is a link analysis algorithm used by google that uses a system of weights attached to each element of a hyperlinked set of documents\"\n","S = \"pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents\"\n","\n","lcs = lcs_norm_word(A, S)\n","print ('LCS = ', lcs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4gjeaIHdSHIY","colab_type":"code","colab":{}},"source":["test_indices = range(5)\n","\n","\n","category_vals = []\n","lcs_norm_vals = []\n","\n","for i in test_indices:\n","  category_vals.append(complete_df.loc[i, 'Category'])\n","  #get texts to compare\n","\n","  answer_text = complete_df.loc[i, 'Text']\n","  task = complete_df.loc[i, 'Task']\n","\n","  orig_rows = complete_df[(complete_df['Class'] == -1)]\n","  orig_row = orig_rows[(orig_rows['Task'] == task)]\n","  source_text = orig_row['Text'].values[0]\n","\n","\n","  ## Calculate lcs\n","\n","  lcs_val = lcs_norm_word(answer_text, source_text)\n","  lcs_norm_vals.append(lcs_val)\n","\n","\n","print ('Original category values: \\n', category_vals)\n","print()\n","print('Normalized LCS values: \\n', lcs_norm_vals)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VY30nO3-0GsI","colab_type":"text"},"source":["# Cosine Similarity "]},{"cell_type":"markdown","metadata":{"id":"BXQnGBOYbzGu","colab_type":"text"},"source":["# Other features"]},{"cell_type":"code","metadata":{"id":"EC5lXz45b3Al","colab_type":"code","colab":{}},"source":["#### Be creative with features --- these are just examples ##############       \n","    \n","#Word Count of the documents – total number of words in the documents\n","#Character Count of the documents – total number of characters in the documents\n","#Average Word Density of the documents – average length of the words used in the documents\n","#Puncutation Count in the Complete Essay – total number of punctuation marks in the documents\n","#Upper Case Count in the Complete Essay – total number of upper count words in the documents\n","#Title Word Count in the Complete Essay – total number of proper case (title) words in the documents\n","#Frequency distribution of Part of Speech Tags:\n","#Noun Count\n","#Verb Count\n","#Adjective Count\n","#Adverb Count\n","#Pronoun Count\n","        \n","    \n","        \n","# Here you can play with features -- add new ones; edit, remove.\n","trainDF['char_count'] = trainDF['text'].apply(len)\n","trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n","trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n","trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n","trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n","trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n","\n","\n","pos_family = {\n","    'noun' : ['NN','NNS','NNP','NNPS'],\n","    'pron' : ['PRP','PRP$','WP','WP$'],\n","    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n","    'adj' :  ['JJ','JJR','JJS'],\n","    'adv' : ['RB','RBR','RBS','WRB']\n","}\n","from textblob import TextBlob\n","# function to check and get the part of speech tag count of a words in a given sentence\n","def check_pos_tag(x, flag):\n","    cnt = 0\n","    try:\n","        wiki = textblob.TextBlob(x)\n","        for tup in wiki.tags:\n","            ppo = list(tup)[1]\n","            if ppo in pos_family[flag]:\n","                cnt += 1\n","    except:\n","        pass\n","    return cnt\n","\n","trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n","trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n","trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n","trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n","trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lRLe7ffxVXCy","colab_type":"text"},"source":["# Create all features"]},{"cell_type":"code","metadata":{"id":"QAXkzHlXV2LX","colab_type":"code","colab":{}},"source":["# Instead of creating another function I could also probs do \n","#trainDF['containment'] = trainDF['text'].apply(lambda x: lcs_norm_word(answer_text, source_text)) \n","# needs some work. For now I will creat function \n","\n","def create_containment_features(df,n, column_name=None):\n","  containment_values= []\n","  if(column_name = None):\n","    column_name = 'c_'+str(n) # c_1, c_2\n","\n","  for i in df.index:\n","    file = df.loc[i, 'ID']\n","    #Computes features using calculate_contaiment function\n","    if df.loc[i,'Category']> -1:\n","      c = calculate_containment(df, n, file)\n","      containment_values.append(-1)\n","\n","  print(str(n)+'-gram containment features created')\n","  return containment_values\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iWN8r80QZfOz","colab_type":"code","colab":{}},"source":["# Function creates lcs feature and add it to the dataframe\n","def create_lcs_features(df, column_name='lcs_word'):\n","    \n","    lcs_values = []\n","    \n","    # iterate through files in dataframe\n","    for i in df.index:\n","        # Computes LCS_norm words feature using function above for answer tasks\n","        if df.loc[i,'Category'] > -1:\n","            # get texts to compare\n","            answer_text = df.loc[i, 'Text'] \n","            task = df.loc[i, 'Task']\n","            # we know that source texts have Class = -1\n","            orig_rows = df[(df['Class'] == -1)]\n","            orig_row = orig_rows[(orig_rows['Task'] == task)]\n","            source_text = orig_row['Text'].values[0]\n","\n","            # calculate lcs\n","            lcs = lcs_norm_word(answer_text, source_text)\n","            lcs_values.append(lcs)\n","        # Sets to -1 for original tasks \n","        else:\n","            lcs_values.append(-1)\n","\n","    print('LCS features created!')\n","    return lcs_values"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cUNKHYW-aUjx","colab_type":"text"},"source":["# Append Features"]},{"cell_type":"code","metadata":{"id":"4vr5FTwKaaZq","colab_type":"code","colab":{}},"source":["# Define an ngram range\n","ngram_range = range(1,7)\n","\n","\n","# The following code may take a minute to run, depending on your ngram_range\n","\"\"\"\n","DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n","\"\"\"\n","features_list = []\n","\n","# Create features in a features_df\n","all_features = np.zeros((len(ngram_range)+1, len(complete_df)))\n","\n","# Calculate features for containment for ngrams in range\n","i=0\n","for n in ngram_range:\n","    column_name = 'c_'+str(n)\n","    features_list.append(column_name)\n","    # create containment features\n","    all_features[i]=np.squeeze(create_containment_features(complete_df, n))\n","    i+=1\n","\n","# Calculate features for LCS_Norm Words \n","features_list.append('lcs_word')\n","all_features[i]= np.squeeze(create_lcs_features(complete_df))\n","\n","# create a features dataframe\n","features_df = pd.DataFrame(np.transpose(all_features), columns=features_list)\n","\n","# Print all features/columns\n","print()\n","print('Features: ', features_list)\n","print()\n","1-gram con"],"execution_count":0,"outputs":[]}]}